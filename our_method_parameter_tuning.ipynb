{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc9de3d",
   "metadata": {},
   "source": [
    "## Tuning the parameters of our model separately for each dataset in the ETH/UCY datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5696522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import dill\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import generative_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2603bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_FDE(pred_x, pred_y, test_x, test_y):\n",
    "\n",
    "    final_displacement_x = pred_x[-1] - test_x[-1]\n",
    "    final_displacement_y = pred_y[-1] - test_y[-1]\n",
    "    FDE = np.sqrt(final_displacement_x**2 + final_displacement_y**2)\n",
    "    \n",
    "    return FDE\n",
    "\n",
    "def calculate_ADE(pred_x, pred_y, test_x, test_y):\n",
    "    total_displacement_error = 0\n",
    "    for point_idx in range(len(test_x)):\n",
    "        displacement_error = np.sqrt((pred_x[point_idx] - test_x[point_idx])**2 + (pred_y[point_idx] - test_y[point_idx])**2)\n",
    "        total_displacement_error += displacement_error\n",
    "\n",
    "    return total_displacement_error/len(pred_x)\n",
    "\n",
    "## The evaluation logic for Trajectron++ loops over the frames and predicts the future trajectories \n",
    "## for each node present in the current frame\n",
    "## Each node has to have at least 7 historical points and 12 future points\n",
    "def get_total_predictable_slices(data):\n",
    "    total_predictable_steps = 0\n",
    "    for i in pd.unique(data.node_id):\n",
    "        #print(len(test[test.node_id == i]))\n",
    "        total_predictable_steps += len(data[data.node_id == i]) - 19\n",
    "    return total_predictable_steps\n",
    "\n",
    "def process_data(input_data, limit=None):\n",
    "    data = input_data.copy()\n",
    "    data['frame_id'] = pd.to_numeric(data['frame_id'], downcast='integer')\n",
    "    data['track_id'] = pd.to_numeric(data['track_id'], downcast='integer')\n",
    "\n",
    "    data['frame_id'] = data['frame_id'] // 10\n",
    "\n",
    "    data['frame_id'] -= data['frame_id'].min()\n",
    "\n",
    "    data['node_type'] = 'PEDESTRIAN'\n",
    "    data['node_id'] = data['track_id'].astype(str)\n",
    "    data.sort_values('frame_id', inplace=True)\n",
    "\n",
    "    data['pos_x'] = data['pos_x'] - data['pos_x'].mean()\n",
    "    data['pos_y'] = data['pos_y'] - data['pos_y'].mean()\n",
    "    \n",
    "    # Select only such nodes which have enough data to predict on (8 historical timesteps, 12 future)\n",
    "    v = data.node_id.value_counts()\n",
    "    data = data[data.node_id.isin(v.index[v.gt(19)])]\n",
    "    if limit:\n",
    "        data = data.iloc[:limit]\n",
    "        data = data[data.node_id.isin(v.index[v.gt(19)])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4494c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_our_method(data, params, dataset_title='', single_output = False):\n",
    "    tot = 0\n",
    "    our_fde_best_of = []\n",
    "    our_ade_best_of = []\n",
    "\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='Ours - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        #print(frame_data)\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check if at least 7 historical points are present\n",
    "            # PS: It might be so that the prediction starts at the 8th step instead of 7th? Edited the code to do this at the moment\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Not sure why there has to be more than 12 frames to the future (at least 13) but it's the\n",
    "                # only way to get the number of trajectron++ eval predictions to match up\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    tot += 1\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    all_pred_x, all_pred_y, _ = generative_model.predict(x_data, y_data, params, trajectory_length=12)\n",
    "\n",
    "                    best_fde = None\n",
    "                    best_ade = None\n",
    "                    \n",
    "                    if single_output:\n",
    "                        avg_x = np.mean(all_pred_x, axis=0)\n",
    "                        avg_y = np.mean(all_pred_y, axis=0)\n",
    "                        best_fde = calculate_FDE(avg_x, avg_y, x_gt, y_gt)\n",
    "                        best_ade = calculate_ADE(avg_x, avg_y, x_gt, y_gt)\n",
    "                    else:\n",
    "                        for i in range(len(all_pred_x)):\n",
    "                            current_pred_x = all_pred_x[i]\n",
    "                            current_pred_y = all_pred_y[i]\n",
    "\n",
    "                            fde = calculate_FDE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                            if best_fde == None or fde < best_fde:\n",
    "                                best_fde = fde\n",
    "\n",
    "                            ade = calculate_ADE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                            if best_ade == None or ade < best_ade:\n",
    "                                best_ade = ade\n",
    "\n",
    "                    our_fde_best_of.append(best_fde)\n",
    "                    our_ade_best_of.append(best_ade)\n",
    "                    \n",
    "    return our_fde_best_of, our_ade_best_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e47d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trajectron_data(trajectron_resultset_name, base_folder='./trajectron++/results_paper_version/', suffix='best_of'):\n",
    "    trajectron_fde = []\n",
    "    with open(base_folder + trajectron_resultset_name + '_fde_' + suffix + '.csv', mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            trajectron_fde.append(float(row['value']))\n",
    "\n",
    "    trajectron_ade = []\n",
    "    with open(base_folder + trajectron_resultset_name + '_ade_'+ suffix + '.csv', mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            trajectron_ade.append(float(row['value']))\n",
    "            \n",
    "    return trajectron_fde, trajectron_ade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5197e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_dataset(our_method_params, file, trajectron_resultset_name, evaluate_most_likely=False, limit=None):\n",
    "    base_path = './raw_data/'\n",
    "\n",
    "    ours_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    trajectron_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "\n",
    "    for param_idx, params in enumerate(our_method_params):\n",
    "        data = pd.read_csv(base_path + file, sep='\\t', index_col=False, header=None)\n",
    "        data.columns = ['frame_id', 'track_id', 'pos_x', 'pos_y']\n",
    "\n",
    "        data = process_data(data, limit=limit)\n",
    "\n",
    "        ## Trajectron\n",
    "        trajectron_fde, trajectron_ade = read_trajectron_data(trajectron_resultset_name)\n",
    "        trajectron_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_fde))\n",
    "        trajectron_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ade))\n",
    "        \n",
    "        if evaluate_most_likely:\n",
    "            trajectron_fde, trajectron_ade = read_trajectron_data(trajectron_resultset_name, suffix='most_likely')\n",
    "            trajectron_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_fde))\n",
    "            trajectron_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ade))\n",
    "\n",
    "        # make sure that there is no discrepancy between our data processing and trajectron evaluation results size\n",
    "        num_predictable_trajectories = get_total_predictable_slices(data)\n",
    "        #assert len(trajectron_fde) == num_predictable_trajectories\n",
    "        #assert len(trajectron_ade) == num_predictable_trajectories\n",
    "\n",
    "        ## Ours\n",
    "        our_fde_best_of_20, our_ade_best_of_20 = evaluate_our_method(data, params, dataset_title=trajectron_resultset_name)\n",
    "        ours_results['BEST_OF_20']['FDE'].append(np.mean(our_fde_best_of_20))\n",
    "        ours_results['BEST_OF_20']['ADE'].append(np.mean(our_ade_best_of_20))\n",
    "        \n",
    "        if evaluate_most_likely:\n",
    "            our_fde_single, our_ade_single = evaluate_our_method(data, params, dataset_title=trajectron_resultset_name, single_output=True)\n",
    "            ours_results['MOST_LIKELY']['FDE'].append(np.mean(our_fde_single))\n",
    "            ours_results['MOST_LIKELY']['ADE'].append(np.mean(our_ade_single))\n",
    "\n",
    "    return [\n",
    "        ours_results,\n",
    "        trajectron_results,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c424e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'eth/test/biwi_eth.txt', \n",
    "    'hotel/test/biwi_hotel.txt', \n",
    "    'zara1/test/crowds_zara01.txt', \n",
    "    'zara2/test/crowds_zara02.txt'\n",
    "]\n",
    "\n",
    "trajectron_resultset_names = [\n",
    "    'eth_vel', \n",
    "    'hotel_vel', \n",
    "    'zara1_vel', \n",
    "    'zara2_vel'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed42f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_param_results(all_params, all_results):\n",
    "    print(\"Best param set idx per dataset (ADE/FDE)\")\n",
    "    for res in all_results:\n",
    "        ours_res = res[0]\n",
    "        ours_bo20_fde = ours_res['BEST_OF_20']['FDE']\n",
    "        ours_bo20_ade = ours_res['BEST_OF_20']['ADE']\n",
    "\n",
    "        index_min_fde = np.argmin(ours_bo20_fde)\n",
    "        index_min_ade = np.argmin(ours_bo20_ade)\n",
    "        print(index_min_fde, \"/\", index_min_ade)\n",
    "\n",
    "\n",
    "    print()\n",
    "    all_param_set_fdes = []\n",
    "    for param_set_no in range(len(z1_res1[0]['BEST_OF_20']['FDE'])):\n",
    "        total_fde = 0\n",
    "        total_ade = 0\n",
    "        for res in all_results:\n",
    "            ours_res = res[0]\n",
    "            ours_bo20_fde = ours_res['BEST_OF_20']['FDE'][param_set_no]\n",
    "            ours_bo20_ade = ours_res['BEST_OF_20']['ADE'][param_set_no]\n",
    "            total_fde += ours_bo20_fde\n",
    "            total_ade += ours_bo20_ade\n",
    "        all_param_set_fdes.append(total_fde)\n",
    "\n",
    "    print(\"Total FDE over all datasets for each param set: \",all_param_set_fdes)\n",
    "    print()\n",
    "    lowest_fde_param_set_idx = np.argmin(all_param_set_fdes)\n",
    "    print(\"Best param set idx: \", lowest_fde_param_set_idx)\n",
    "    print(\"Param set: \", all_params[lowest_fde_param_set_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a026578",
   "metadata": {},
   "source": [
    "## Zara 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56dab72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_params = []\n",
    "\n",
    "grouping_data = [\n",
    "    #([0.1, 0.4, 0.65, 0.85, 1.0], [1, 6, 5, 5, 3]), \n",
    "    #([0.05, 0.35, 0.7, 0.9, 1.0], [1, 8, 5, 3, 3]), \n",
    "    #([0.1, 0.35, 0.7, 0.9, 1.0], [1, 8, 5, 4, 2]), \n",
    "    #([0.05, 0.25, 0.5, 0.75, 1.0], [1, 5, 5, 5, 4]),\n",
    "    #([0.05, 0.30, 0.60, 0.90, 1.0], [1, 6, 5, 5, 3]),\n",
    "    #([0.05, 0.25, 0.5, 1.0], [1, 7, 7, 5]),\n",
    "    #([0.05, 0.33, 0.66, 1.0], [1, 7, 7, 5]),\n",
    "    #([0.1, 0.33, 0.66, 1.0], [1, 8, 6, 5]),\n",
    "    #([0.05, 0.5, 0.75, 1.0], [1, 9, 6, 4]),\n",
    "    #([0.05, 0.60, 1.0], [1, 11, 8]),\n",
    "    ([0.05, 0.60, 1.0], [1, 14, 5]), # done\n",
    "    #([0.1, 0.70, 1.0], [1, 12, 7]),\n",
    "    #([0.05, 1.0], [1, 19]),\n",
    "    #([1.0], [20]),\n",
    "]\n",
    "\n",
    "for angle_change_prob in [0.2]: #done, high value\n",
    "    for angle_change_noise in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:\n",
    "        for velocity_change_prob in [0.1]: #done, mid value\n",
    "            for velocity_change_noise in [0.1]: #done, low value\n",
    "                for discount_lower_bound in [0.2]: #done, low value\n",
    "                    for discount_avg_prob in [1.0]: #done, high value\n",
    "                        for group_data in grouping_data:\n",
    "                            for noise in [0.05]: #done, mid value\n",
    "                                for stop_prob in [0.0]: #done, low value\n",
    "                                    for const_vel_model_prob in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "                                        group_p, group_c_count = group_data\n",
    "                                        one_param_set = {\n",
    "                                            'NOISE': noise, \n",
    "                                            'NO_OF_TRAJECTORIES': 300, \n",
    "                                            'CONST_VEL_MODEL_PROB': const_vel_model_prob, \n",
    "                                            'STOP_PROB': stop_prob, \n",
    "                                            'DISCOUNT_AVG_PROB': discount_avg_prob, \n",
    "                                            'DISCOUNT_LOWER_BOUND': discount_lower_bound, \n",
    "                                            'VELOCITY_CHANGE_PROB': velocity_change_prob,\n",
    "                                            'VELOCITY_CHANGE_NOISE': velocity_change_noise, \n",
    "                                            'ANGLE_CHANGE_PROB': angle_change_prob, \n",
    "                                            'ANGLE_CHANGE_NOISE': angle_change_noise, \n",
    "                                            'GROUP_PERCENTAGES': group_p, \n",
    "                                            'GROUP_CLUSTER_COUNT': group_c_count\n",
    "                                        }\n",
    "\n",
    "\n",
    "                                        z1_params.append(one_param_set)\n",
    "                        \n",
    "z1_final_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 300, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.0, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.2, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 14, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:46<00:00,  5.05it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:45<00:00,  5.17it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:41<00:00,  5.58it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.27it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.29it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:42<00:00,  5.44it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:45<00:00,  5.18it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:45<00:00,  5.11it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.23it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.35it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.28it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.38it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.25it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.24it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.25it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.33it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.27it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.33it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.23it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.38it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.31it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.42it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:42<00:00,  5.49it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:40<00:00,  5.75it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.42it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.36it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.43it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:42<00:00,  5.46it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:45<00:00,  5.19it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.22it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:43<00:00,  5.40it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:44<00:00,  5.30it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:42<00:00,  5.46it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:42<00:00,  5.47it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:42<00:00,  5.52it/s]\n",
      "Ours - zara2_vel_modified: 100%|██████████████████████████████████████████████████████████████| 234/234 [00:45<00:00,  5.17it/s]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:53<00:00,  1.48s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:52<00:00,  1.47s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:52<00:00,  1.46s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:52<00:00,  1.47s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:49<00:00,  1.38s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:52<00:00,  1.47s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:54<00:00,  1.51s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:53<00:00,  1.48s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:52<00:00,  1.47s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:53<00:00,  1.48s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:53<00:00,  1.48s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:53<00:00,  1.49s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:53<00:00,  1.50s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:53<00:00,  1.49s/it]\n",
      "Ours - univ_vel: 100%|██████████████████████████████████████████████████████████████████████████| 36/36 [00:53<00:00,  1.47s/it]\n",
      "Ours - univ_vel:  33%|████████████████████████▋                                                 | 12/36 [00:17<00:41,  1.73s/it]"
     ]
    }
   ],
   "source": [
    "z1_res1 = evaluate_on_dataset(z1_params, 'zara2/test/crowds_zara02.txt', 'zara2_vel_modified', limit=1600, evaluate_most_likely=False)\n",
    "z1_res2 = evaluate_on_dataset(z1_params, 'univ/test/students001.txt', 'univ_vel', limit=2200, evaluate_most_likely=False)\n",
    "z1_res3 = evaluate_on_dataset(z1_params, 'hotel/test/biwi_hotel.txt', 'hotel_vel', limit=1600, evaluate_most_likely=False)\n",
    "z1_res4 = evaluate_on_dataset(z1_params, 'eth/test/biwi_eth.txt', 'eth_vel', limit=None, evaluate_most_likely=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "37c40ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param set idx per dataset (ADE/FDE)\n",
      "1 / 1\n",
      "3 / 3\n",
      "2 / 2\n",
      "0 / 0\n",
      "\n",
      "Total FDE over all datasets for each param set:  [1.7669317442270898, 1.8393056515320971, 1.9338614258390265, 2.0206393648785106, 2.1753698036771283, 2.434702511613644]\n",
      "\n",
      "Best param set idx:  0\n",
      "Param set:  {'NOISE': 0.05, 'NO_OF_TRAJECTORIES': 300, 'CONST_VEL_MODEL_PROB': 0.5, 'STOP_PROB': 0.0, 'DISCOUNT_AVG_PROB': 1.0, 'DISCOUNT_LOWER_BOUND': 0.2, 'VELOCITY_CHANGE_PROB': 0.1, 'VELOCITY_CHANGE_NOISE': 0.1, 'ANGLE_CHANGE_PROB': 0.2, 'ANGLE_CHANGE_NOISE': 2, 'GROUP_PERCENTAGES': [0.05, 0.6, 1.0], 'GROUP_CLUSTER_COUNT': [1, 14, 5]}\n"
     ]
    }
   ],
   "source": [
    "all_z1_res = [z1_res1, z1_res2, z1_res3, z1_res4]\n",
    "check_param_results(z1_params, all_z1_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc746d",
   "metadata": {},
   "source": [
    "## Zara2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2_params = []\n",
    "\n",
    "grouping_data = [\n",
    "    ([0.05, 0.60, 1.0], [1, 14, 5]),\n",
    "]\n",
    "\n",
    "for angle_change_prob in [0.1]: #done, mid value\n",
    "    for angle_change_noise in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:\n",
    "        for velocity_change_prob in [0.1]: #done, mid value\n",
    "            for velocity_change_noise in [0.1]: #done, low value\n",
    "                for discount_lower_bound in [0.2]: #done, low value\n",
    "                    for discount_avg_prob in [1.0]: #done, high value\n",
    "                        for group_data in grouping_data:\n",
    "                            for noise in [0.1]: #done, mid value\n",
    "                                for stop_prob in [0.025]: #done, mid value\n",
    "                                    for const_vel_model_prob in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "                                        group_p, group_c_count = group_data\n",
    "                                        one_param_set = {\n",
    "                                            'NOISE': noise, \n",
    "                                            'NO_OF_TRAJECTORIES': 300, \n",
    "                                            'CONST_VEL_MODEL_PROB': const_vel_model_prob, \n",
    "                                            'STOP_PROB': stop_prob, \n",
    "                                            'DISCOUNT_AVG_PROB': discount_avg_prob, \n",
    "                                            'DISCOUNT_LOWER_BOUND': discount_lower_bound, \n",
    "                                            'VELOCITY_CHANGE_PROB': velocity_change_prob,\n",
    "                                            'VELOCITY_CHANGE_NOISE': velocity_change_noise, \n",
    "                                            'ANGLE_CHANGE_PROB': angle_change_prob, \n",
    "                                            'ANGLE_CHANGE_NOISE': angle_change_noise, \n",
    "                                            'GROUP_PERCENTAGES': group_p, \n",
    "                                            'GROUP_CLUSTER_COUNT': group_c_count\n",
    "                                        }\n",
    "\n",
    "\n",
    "                                        z2_params.append(one_param_set)\n",
    "                        \n",
    "z2_final_params = {\n",
    "    'NOISE': 0.1, \n",
    "    'NO_OF_TRAJECTORIES': 200, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.025, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.2, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.1, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 14, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2_res1 = evaluate_on_dataset(z2_params, 'zara1/test/crowds_zara01.txt', 'zara1_vel_modified', limit=1400, evaluate_most_likely=False)\n",
    "z2_res2 = evaluate_on_dataset(z2_params, 'univ/test/students001.txt', 'univ_vel', limit=2200, evaluate_most_likely=False)\n",
    "z2_res3 = evaluate_on_dataset(z2_params, 'hotel/test/biwi_hotel.txt', 'hotel_vel', limit=1600, evaluate_most_likely=False)\n",
    "z2_res4 = evaluate_on_dataset(z2_params, 'eth/test/biwi_eth.txt', 'eth_vel', limit=None, evaluate_most_likely=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_z2_res = [z2_res1, z2_res2, z2_res3, z2_res4]\n",
    "check_param_results(z2_params, all_z2_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05a932",
   "metadata": {},
   "source": [
    "## Univ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a792fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_params = []\n",
    "\n",
    "grouping_data = [\n",
    "    ([0.05, 0.5, 0.75, 1.0], [1, 9, 6, 4]),\n",
    "]\n",
    "\n",
    "for angle_change_prob in [0.2]: #done, high value\n",
    "    for angle_change_noise in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:\n",
    "        for velocity_change_prob in [0.1]: #done, mid value\n",
    "            for velocity_change_noise in [0.1]: #done, low value\n",
    "                for discount_lower_bound in [0.1]: #done, low value\n",
    "                    for discount_avg_prob in [1.0]: #done, high value\n",
    "                        for group_data in grouping_data:\n",
    "                            for noise in [0.025]: #done, low value\n",
    "                                for stop_prob in [0.05]: #done, mid value\n",
    "                                    for const_vel_model_prob in [0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "                                        group_p, group_c_count = group_data\n",
    "                                        one_param_set = {\n",
    "                                            'NOISE': noise, \n",
    "                                            'NO_OF_TRAJECTORIES': 300, \n",
    "                                            'CONST_VEL_MODEL_PROB': const_vel_model_prob, \n",
    "                                            'STOP_PROB': stop_prob, \n",
    "                                            'DISCOUNT_AVG_PROB': discount_avg_prob, \n",
    "                                            'DISCOUNT_LOWER_BOUND': discount_lower_bound, \n",
    "                                            'VELOCITY_CHANGE_PROB': velocity_change_prob,\n",
    "                                            'VELOCITY_CHANGE_NOISE': velocity_change_noise, \n",
    "                                            'ANGLE_CHANGE_PROB': angle_change_prob, \n",
    "                                            'ANGLE_CHANGE_NOISE': angle_change_noise, \n",
    "                                            'GROUP_PERCENTAGES': group_p, \n",
    "                                            'GROUP_CLUSTER_COUNT': group_c_count\n",
    "                                        }\n",
    "\n",
    "\n",
    "                                        univ_params.append(one_param_set)\n",
    "                        \n",
    "univ_final_params = {\n",
    "    'NOISE': 0.025, \n",
    "    'NO_OF_TRAJECTORIES': 200, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.05, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.1, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.5, 0.75, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 9, 6, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_res1 = evaluate_on_dataset(univ_params, 'zara1/test/crowds_zara01.txt', 'zara1_vel_modified', limit=1400, evaluate_most_likely=False)\n",
    "univ_res2 = evaluate_on_dataset(univ_params, 'zara2/test/crowds_zara02.txt', 'zara2_vel_modified', limit=1600, evaluate_most_likely=False)\n",
    "univ_res3 = evaluate_on_dataset(univ_params, 'hotel/test/biwi_hotel.txt', 'hotel_vel', limit=1600, evaluate_most_likely=False)\n",
    "univ_res4 = evaluate_on_dataset(univ_params, 'eth/test/biwi_eth.txt', 'eth_vel', limit=None, evaluate_most_likely=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_univ_res = [univ_res1, univ_res2, univ_res3, univ_res4]\n",
    "check_param_results(univ_params, all_univ_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042391d",
   "metadata": {},
   "source": [
    "## Hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccead9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_params = []\n",
    "\n",
    "grouping_data = [\n",
    "    ([0.05, 0.60, 1.0], [1, 11, 8]),\n",
    "]\n",
    "\n",
    "for angle_change_prob in [0.2]: #done, high value\n",
    "    for angle_change_noise in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:\n",
    "        for velocity_change_prob in [0.1]: #done, mid value\n",
    "            for velocity_change_noise in [0.1]: #done, low value\n",
    "                for discount_lower_bound in [0.1]: #done, low value\n",
    "                    for discount_avg_prob in [1.0]: #done, high value\n",
    "                        for group_data in grouping_data:\n",
    "                            for noise in [0.05]: #done, mid value\n",
    "                                for stop_prob in [0.0]: #done, low value\n",
    "                                    for const_vel_model_prob in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "                                        const_vel_model_prob\n",
    "                                        group_p, group_c_count = group_data\n",
    "                                        one_param_set = {\n",
    "                                            'NOISE': noise, \n",
    "                                            'NO_OF_TRAJECTORIES': 300, \n",
    "                                            'CONST_VEL_MODEL_PROB': const_vel_model_prob, \n",
    "                                            'STOP_PROB': stop_prob, \n",
    "                                            'DISCOUNT_AVG_PROB': discount_avg_prob, \n",
    "                                            'DISCOUNT_LOWER_BOUND': discount_lower_bound, \n",
    "                                            'VELOCITY_CHANGE_PROB': velocity_change_prob,\n",
    "                                            'VELOCITY_CHANGE_NOISE': velocity_change_noise, \n",
    "                                            'ANGLE_CHANGE_PROB': angle_change_prob, \n",
    "                                            'ANGLE_CHANGE_NOISE': angle_change_noise, \n",
    "                                            'GROUP_PERCENTAGES': group_p, \n",
    "                                            'GROUP_CLUSTER_COUNT': group_c_count\n",
    "                                        }\n",
    "\n",
    "\n",
    "                                        hotel_params.append(one_param_set)\n",
    "                        \n",
    "hotel_final_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 200, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.0, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.1, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 11, 8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333697fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_res1 = evaluate_on_dataset(hotel_params, 'zara1/test/crowds_zara01.txt', 'zara1_vel_modified', limit=1400, evaluate_most_likely=False)\n",
    "hotel_res2 = evaluate_on_dataset(hotel_params, 'zara2/test/crowds_zara02.txt', 'zara2_vel_modified', limit=1600, evaluate_most_likely=False)\n",
    "hotel_res3 = evaluate_on_dataset(hotel_params, 'univ/test/students001.txt', 'univ_vel', limit=2200, evaluate_most_likely=False)\n",
    "hotel_res4 = evaluate_on_dataset(hotel_params, 'eth/test/biwi_eth.txt', 'eth_vel', limit=None, evaluate_most_likely=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2daebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hotel_res = [hotel_res1, hotel_res2, hotel_res3, hotel_res4]\n",
    "check_param_results(hotel_params, all_hotel_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cc445",
   "metadata": {},
   "source": [
    "## ETH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_params = []\n",
    "\n",
    "grouping_data = [\n",
    "    ([0.05, 0.5, 0.75, 1.0], [1, 9, 6, 4]),\n",
    "]\n",
    "\n",
    "for angle_change_prob in [0.2]: #done, high value\n",
    "    for angle_change_noise in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:\n",
    "        for velocity_change_prob in [0.1]: #done, mid value\n",
    "            for velocity_change_noise in [0.1]: #done, low value\n",
    "                for discount_lower_bound in [0.15]: #done, mid value\n",
    "                    for discount_avg_prob in [1.0]: #done, high value\n",
    "                        for group_data in grouping_data:\n",
    "                            for noise in [0.05]: #done, mid value\n",
    "                                for stop_prob in [0.025]: #done, mid value\n",
    "                                    for const_vel_model_prob in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "\n",
    "                                        group_p, group_c_count = group_data\n",
    "                                        one_param_set = {\n",
    "                                            'NOISE': noise, \n",
    "                                            'NO_OF_TRAJECTORIES': 300, \n",
    "                                            'CONST_VEL_MODEL_PROB': const_vel_model_prob, \n",
    "                                            'STOP_PROB': stop_prob, \n",
    "                                            'DISCOUNT_AVG_PROB': discount_avg_prob, \n",
    "                                            'DISCOUNT_LOWER_BOUND': discount_lower_bound, \n",
    "                                            'VELOCITY_CHANGE_PROB': velocity_change_prob,\n",
    "                                            'VELOCITY_CHANGE_NOISE': velocity_change_noise, \n",
    "                                            'ANGLE_CHANGE_PROB': angle_change_prob, \n",
    "                                            'ANGLE_CHANGE_NOISE': angle_change_noise, \n",
    "                                            'GROUP_PERCENTAGES': group_p, \n",
    "                                            'GROUP_CLUSTER_COUNT': group_c_count\n",
    "                                        }\n",
    "\n",
    "\n",
    "                                        eth_params.append(one_param_set)\n",
    "                        \n",
    "eth_final_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 200, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.025, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.15, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.5, 0.75, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 9, 6, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfe04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_res1 = evaluate_on_dataset(hotel_params, 'zara1/test/crowds_zara01.txt', 'zara1_vel_modified', limit=1400, evaluate_most_likely=False)\n",
    "eth_res2 = evaluate_on_dataset(hotel_params, 'zara2/test/crowds_zara02.txt', 'zara2_vel_modified', limit=1600, evaluate_most_likely=False)\n",
    "eth_res3 = evaluate_on_dataset(hotel_params, 'univ/test/students001.txt', 'univ_vel', limit=2200, evaluate_most_likely=False)\n",
    "eth_res4 = evaluate_on_dataset(hotel_params, 'hotel/test/biwi_hotel.txt', 'hotel_vel', limit=1600, evaluate_most_likely=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eth_res = [eth_res1, eth_res2, eth_res3, eth_res4]\n",
    "check_param_results(eth_params, all_eth_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc1c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
