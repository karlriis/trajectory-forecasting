{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef8f5cc-aca1-4d58-9c4f-0f60d3b50a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839bbd40-ee5d-463c-bcda-ec202fb320a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kriis/miniforge3/envs/tensorflow_m1/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading:./OpenTraj/datasets/Edinburgh/annotations/tracks.01Sep.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2342/2342 [01:03<00:00, 36.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! too big dt in [Edinburgh]\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "from data_processing import read_edinburgh_data \n",
    "# set sampling rate as 9 so that the velocities in the df are correct\n",
    "edinburgh_data, edinburgh_agent_ids = read_edinburgh_data(sampling_rate=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551a4fe9-a1a2-455a-89ee-b44479f16e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import read_eth_data\n",
    "eth_data, eth_agent_ids = read_eth_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17633c-1ac4-4ab8-aa64-a8bc7e85e42b",
   "metadata": {},
   "source": [
    "### Neural network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dff87b-2a63-4d4d-b5c3-fab94a0c2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import backend as K\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "model = models.load_model('models/Edinburgh_NN_5_point_pred_velocity', custom_objects={\"RMSE\": RMSE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3edc97-3a52-4367-a020-260d1e8dacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edinburgh_test_agent_ids = np.load('test_agent_ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "520cffdb-52ee-472c-a8a9-2f8bf640e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_prediction(data, agent_id):\n",
    "    seen_x = np.array(data[data['agent_id'] == agent_id].iloc[:5]['pos_x'])\n",
    "    seen_y = np.array(data[data['agent_id'] == agent_id].iloc[:5]['pos_y'])\n",
    "    \n",
    "    seen_vel_x = np.array(data[data['agent_id'] == agent_id].iloc[1:5]['vel_x'])\n",
    "    seen_vel_y = np.array(data[data['agent_id'] == agent_id].iloc[1:5]['vel_y'])\n",
    "    \n",
    "    unseen_x = np.array(data[data['agent_id'] == agent_id].iloc[5:10]['pos_x'])\n",
    "    unseen_y = np.array(data[data['agent_id'] == agent_id].iloc[5:10]['pos_y'])\n",
    "\n",
    "    pred_vel_x, pred_vel_y = model.predict(np.array([np.column_stack((seen_vel_x, seen_vel_y))]))\n",
    "    pred_vel_x = pred_vel_x[0] # destructure the prediction array as it is [[x1, x2, x3...]]\n",
    "    pred_vel_y = pred_vel_y[0]\n",
    "    \n",
    "    pred_coord_x = [seen_x[-1] + pred_vel_x[0]]\n",
    "    pred_coord_y = [seen_y[-1] + pred_vel_y[0]]\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        pred_coord_x.append(pred_coord_x[-1] + pred_vel_x[i])\n",
    "        pred_coord_y.append(pred_coord_y[-1] + pred_vel_y[i])\n",
    "\n",
    "    FDE = np.sqrt((pred_coord_x[-1] - unseen_x[-1])**2 + (pred_coord_y[-1] - unseen_y[-1])**2)\n",
    "    return pred_coord_x, pred_coord_y, FDE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5007745-5abb-44c3-ba4f-87da370abf1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9f5001a-a40c-4b95-a67b-a2070ed26d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data, begin_idx, agent_id, num_steps=5):\n",
    "    sample_x = data[\"pos_x\"].loc[data[\"agent_id\"]==agent_id][begin_idx:begin_idx+num_steps].values\n",
    "    sample_y = data[\"pos_y\"].loc[data[\"agent_id\"]==agent_id][begin_idx:begin_idx+num_steps].values\n",
    "\n",
    "    test_x = data[\"pos_x\"].loc[data[\"agent_id\"]==agent_id][begin_idx+num_steps:begin_idx+2*num_steps].values\n",
    "    test_y = data[\"pos_y\"].loc[data[\"agent_id\"]==agent_id][begin_idx+num_steps:begin_idx+2*num_steps].values\n",
    "    \n",
    "    return sample_x, sample_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1f704b6-eeda-4d6e-832d-eff73cfad003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vel_const(data, agent_id):\n",
    "    sample_x, sample_y, test_x, test_y = generate_data(data, 0, agent_id, 5)\n",
    "    \n",
    "    sample_vel_x = [sample_x[i] - sample_x[i-1] for i in range(1, len(sample_x))]\n",
    "    sample_vel_y = [sample_y[i] - sample_y[i-1] for i in range(1, len(sample_y))]\n",
    "    \n",
    "    avg_vel_x = np.mean(sample_vel_x)\n",
    "    avg_vel_y = np.mean(sample_vel_y)\n",
    "    \n",
    "    pred_x = [sample_x[-1] + i*avg_vel_x for i in range(1, 6)]\n",
    "    pred_y = [sample_y[-1] + i*avg_vel_y for i in range(1, 6)]\n",
    "    \n",
    "    FDE = np.sqrt((pred_x[-1] - test_x[-1])**2 + (pred_y[-1] - test_y[-1])**2)\n",
    "    return FDE\n",
    "\n",
    "def last_vel_const(data, agent_id):\n",
    "    sample_x, sample_y, test_x, test_y = generate_data(data, 0, agent_id, 5)\n",
    "    \n",
    "    sample_vel_x = [sample_x[i] - sample_x[i-1] for i in range(1, len(sample_x))]\n",
    "    sample_vel_y = [sample_y[i] - sample_y[i-1] for i in range(1, len(sample_y))]\n",
    "    \n",
    "    last_vel_x = sample_vel_x[-1]\n",
    "    last_vel_y = sample_vel_y[-1]\n",
    "    \n",
    "    pred_x = [sample_x[-1] + i*last_vel_x for i in range(1, 6)]\n",
    "    pred_y = [sample_y[-1] + i*last_vel_y for i in range(1, 6)]\n",
    "    \n",
    "    FDE = np.sqrt((pred_x[-1] - test_x[-1])**2 + (pred_y[-1] - test_y[-1])**2)\n",
    "    return FDE\n",
    "\n",
    "def discount_vel_const(data, agent_id):\n",
    "    sample_x, sample_y, test_x, test_y = generate_data(data, 0, agent_id, 5)\n",
    "    \n",
    "    sample_vel_x = [sample_x[i] - sample_x[i-1] for i in range(1, len(sample_x))]\n",
    "    sample_vel_y = [sample_y[i] - sample_y[i-1] for i in range(1, len(sample_y))]\n",
    "    \n",
    "    disc_avg_x = (0.8**3*sample_vel_x[0] + 0.8**2*sample_vel_x[1] + 0.8*sample_vel_x[2] + sample_vel_x[3]) / (0.8**3 + 0.8**2 + 0.8 + 1)\n",
    "    disc_avg_y = (0.8**3*sample_vel_y[0] + 0.8**2*sample_vel_y[1] + 0.8*sample_vel_y[2] + sample_vel_y[3]) / (0.8**3 + 0.8**2 + 0.8 + 1)\n",
    "    \n",
    "    pred_x = [sample_x[-1] + i*disc_avg_x for i in range(1, 6)]\n",
    "    pred_y = [sample_y[-1] + i*disc_avg_y for i in range(1, 6)]\n",
    "    \n",
    "    FDE = np.sqrt((pred_x[-1] - test_x[-1])**2 + (pred_y[-1] - test_y[-1])**2)\n",
    "    return FDE\n",
    "\n",
    "def avg_acc_const(data, agent_id):\n",
    "    sample_x, sample_y, test_x, test_y = generate_data(data, 0, agent_id, 5)\n",
    "    \n",
    "    sample_vel_x = [sample_x[i] - sample_x[i-1] for i in range(1, len(sample_x))]\n",
    "    sample_vel_y = [sample_y[i] - sample_y[i-1] for i in range(1, len(sample_y))]\n",
    "    \n",
    "    sample_acceleration_x = [sample_vel_x[i] - sample_vel_x[i-1] for i in range(1, len(sample_vel_x))]\n",
    "    sample_acceleration_y = [sample_vel_y[i] - sample_vel_y[i-1] for i in range(1, len(sample_vel_y))]\n",
    "    \n",
    "    avg_acceleration_x = np.mean(sample_acceleration_x)\n",
    "    avg_acceleration_y = np.mean(sample_acceleration_y)\n",
    "\n",
    "    last_vel_x = sample_vel_x[-1]\n",
    "    last_vel_y = sample_vel_y[-1]\n",
    "    \n",
    "    pred_x = [sample_x[-1] + last_vel_x + avg_acceleration_x]\n",
    "    pred_y = [sample_y[-1] + last_vel_y + avg_acceleration_y]\n",
    "    for i in range(1, 5):\n",
    "        pred_x.append(pred_x[i-1] + last_vel_x + i * avg_acceleration_x)\n",
    "        pred_y.append(pred_y[i-1] + last_vel_y + i * avg_acceleration_y)\n",
    "        \n",
    "    FDE = np.sqrt((pred_x[-1] - test_x[-1])**2 + (pred_y[-1] - test_y[-1])**2)\n",
    "    return FDE\n",
    "        \n",
    "def last_acc_const(data, agent_id):\n",
    "    sample_x, sample_y, test_x, test_y = generate_data(data, 0, agent_id, 5)\n",
    "    \n",
    "    sample_vel_x = [sample_x[i] - sample_x[i-1] for i in range(1, len(sample_x))]\n",
    "    sample_vel_y = [sample_y[i] - sample_y[i-1] for i in range(1, len(sample_y))]\n",
    "    \n",
    "    avg_vel_x = np.mean(sample_vel_x)\n",
    "    avg_vel_y = np.mean(sample_vel_y)\n",
    "    \n",
    "    sample_acceleration_x = [sample_vel_x[i] - sample_vel_x[i-1] for i in range(1, len(sample_vel_x))]\n",
    "    sample_acceleration_y = [sample_vel_y[i] - sample_vel_y[i-1] for i in range(1, len(sample_vel_y))]\n",
    "    \n",
    "    last_acceleration_x = sample_acceleration_x[-1]\n",
    "    last_acceleration_y = sample_acceleration_y[-1]\n",
    "    \n",
    "    last_vel_x = sample_vel_x[-1]\n",
    "    last_vel_y = sample_vel_y[-1]\n",
    "    \n",
    "    pred_x = [sample_x[-1] + last_vel_x + last_acceleration_x]\n",
    "    pred_y = [sample_y[-1] + last_vel_y + last_acceleration_y]\n",
    "    for i in range(1, 5):\n",
    "        pred_x.append(pred_x[i-1] + last_vel_x + i * last_acceleration_x)\n",
    "        pred_y.append(pred_y[i-1] + last_vel_y + i * last_acceleration_y)\n",
    "        \n",
    "    FDE = np.sqrt((pred_x[-1] - test_x[-1])**2 + (pred_y[-1] - test_y[-1])**2)\n",
    "    return FDE\n",
    "\n",
    "def discount_acc_const(data, agent_id):\n",
    "    sample_x, sample_y, test_x, test_y = generate_data(data, 0, agent_id, 5)\n",
    "    \n",
    "    sample_vel_x = [sample_x[i] - sample_x[i-1] for i in range(1, len(sample_x))]\n",
    "    sample_vel_y = [sample_y[i] - sample_y[i-1] for i in range(1, len(sample_y))]\n",
    "    \n",
    "    avg_speed_x = np.mean(sample_vel_x)\n",
    "    avg_speed_y = np.mean(sample_vel_y)\n",
    "    \n",
    "    sample_acc_x = [sample_vel_x[i] - sample_vel_x[i-1] for i in range(1, len(sample_vel_x))]\n",
    "    sample_acc_y = [sample_vel_y[i] - sample_vel_y[i-1] for i in range(1, len(sample_vel_y))]\n",
    "    \n",
    "    discounted_avg_acc_x = (0.8**2*sample_acc_x[0] + 0.8*sample_acc_x[1] + sample_acc_x[2]) / (0.8**2 + 0.8 + 1)\n",
    "    discounted_avg_acc_y = (0.8**2*sample_acc_y[0] + 0.8*sample_acc_y[1] + sample_acc_y[2]) / (0.8**2 + 0.8 + 1)\n",
    "\n",
    "    last_vel_x = sample_vel_x[-1]\n",
    "    last_vel_y = sample_vel_y[-1]\n",
    "    \n",
    "    pred_x = [sample_x[-1] + last_vel_x + discounted_avg_acc_x]\n",
    "    pred_y = [sample_y[-1] + last_vel_y + discounted_avg_acc_y]\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        pred_x.append(pred_x[i-1] + last_vel_x + i * discounted_avg_acc_x)\n",
    "        pred_y.append(pred_y[i-1] + last_vel_y + i * discounted_avg_acc_y)\n",
    "\n",
    "\n",
    "    FDE = np.sqrt((pred_x[-1] - test_x[-1])**2 + (pred_y[-1] - test_y[-1])**2)\n",
    "    return FDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38547096-d0f0-42f2-bed0-f90aafd63fb6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "89eddbd7-a0ed-45e3-87e5-a91e49d0abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(data, test_agent_ids):\n",
    "    NN_edinburgh_FDE = []\n",
    "    avg_vel_const_FDE = []\n",
    "    last_vel_const_FDE = []\n",
    "    discount_vel_const_FDE = []\n",
    "    avg_acc_const_FDE = []\n",
    "    ast_acc_const_FDE = []\n",
    "    discount_acc_const_FDE = []\n",
    "\n",
    "    for idx, agent_id in enumerate(test_agent_ids):\n",
    "        _, _, edinburgh_FDE = NN_prediction(data, agent_id)\n",
    "        NN_edinburgh_FDE.append(edinburgh_FDE)\n",
    "        avg_vel_const_FDE.append(avg_vel_const(edinburgh_data, agent_id))\n",
    "        last_vel_const_FDE.append(last_vel_const(edinburgh_data, agent_id))\n",
    "        discount_vel_const_FDE.append(discount_vel_const(edinburgh_data, agent_id))\n",
    "        avg_acc_const_FDE.append(avg_acc_const(edinburgh_data, agent_id))\n",
    "        last_acc_const_FDE.append(last_acc_const(edinburgh_data, agent_id))\n",
    "        discount_acc_const_FDE.append(discount_acc_const(edinburgh_data, agent_id))\n",
    "    \n",
    "    res_FDE = [\n",
    "        np.mean(NN_edinburgh_FDE),\n",
    "        np.mean(avg_vel_const_FDE),\n",
    "        np.mean(last_vel_const_FDE),\n",
    "        np.mean(discount_vel_const_FDE),\n",
    "        np.mean(avg_acc_const_FDE),\n",
    "        np.mean(last_acc_const_FDE),\n",
    "        np.mean(discount_acc_const_FDE)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0aa38182-8a71-48ed-b723-68226985b5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0k/4cxqtr6n2bq_bpqp64nc__rc0000gn/T/ipykernel_76016/496433689.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0medinburgh_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medinburgh_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medinburgh_test_agent_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0meth_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meth_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meth_agent_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/0k/4cxqtr6n2bq_bpqp64nc__rc0000gn/T/ipykernel_76016/3918879627.py\u001b[0m in \u001b[0;36mrun_models\u001b[0;34m(data, test_agent_ids)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdiscount_vel_const_FDE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount_vel_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medinburgh_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mavg_acc_const_FDE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_acc_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medinburgh_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlast_acc_const_FDE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_acc_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medinburgh_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdiscount_acc_const_FDE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount_acc_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medinburgh_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "index = [\n",
    "    'Neural network',\n",
    "    'Average velocity as const',\n",
    "    'Last velocity as const',\n",
    "    'Discounted velocity as const',\n",
    "    'Average acceleration as const',\n",
    "    'Last acceleration as const',\n",
    "    'Discounted acceleration as const'\n",
    "]\n",
    "\n",
    "edinburgh_series = run_models(edinburgh_data, edinburgh_test_agent_ids)\n",
    "\n",
    "eth_series = run_models(eth_data, eth_agent_ids)\n",
    "\n",
    "d = {'Edinburgh' : pd.Series(edinburgh_series,\n",
    "                       index = index),\n",
    "      'ETH' : pd.Series(eth_series,\n",
    "                        index = index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7632c1d7-45a5-4496-b335-6e9108d6a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb71a38-b6a1-4e70-ac1d-722cd71e7878",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74170dc5-345f-4a22-88c7-67812c645311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edinburgh</th>\n",
       "      <th>ETH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neural network</th>\n",
       "      <td>2.209424</td>\n",
       "      <td>1.843934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average velocity as const</th>\n",
       "      <td>2.472702</td>\n",
       "      <td>0.816345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Last velocity as const</th>\n",
       "      <td>2.140014</td>\n",
       "      <td>0.675217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discounted velocity as const</th>\n",
       "      <td>2.318411</td>\n",
       "      <td>0.733045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average acceleration as const</th>\n",
       "      <td>3.976010</td>\n",
       "      <td>1.179001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Last acceleration as const</th>\n",
       "      <td>4.992026</td>\n",
       "      <td>2.245425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discounted acceleration as const</th>\n",
       "      <td>3.839182</td>\n",
       "      <td>1.175476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Edinburgh       ETH\n",
       "Neural network                     2.209424  1.843934\n",
       "Average velocity as const          2.472702  0.816345\n",
       "Last velocity as const             2.140014  0.675217\n",
       "Discounted velocity as const       2.318411  0.733045\n",
       "Average acceleration as const      3.976010  1.179001\n",
       "Last acceleration as const         4.992026  2.245425\n",
       "Discounted acceleration as const   3.839182  1.175476"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab9df9-6dd5-4164-85db-31033745c882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
