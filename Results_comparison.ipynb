{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1740c3",
   "metadata": {},
   "source": [
    "# Results comparison between Trajectron++ and our method\n",
    "Comparing the results of our method with the Trajectron++ paper's method. The goal of the comparison is to evaluate Trajectron++ on a different dataset than it was trained on to allow more fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa130bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import dill\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import generative_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd1a8c",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac04490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_FDE(pred_x, pred_y, test_x, test_y):\n",
    "\n",
    "    final_displacement_x = pred_x[-1] - test_x[-1]\n",
    "    final_displacement_y = pred_y[-1] - test_y[-1]\n",
    "    FDE = np.sqrt(final_displacement_x**2 + final_displacement_y**2)\n",
    "    \n",
    "    return FDE\n",
    "\n",
    "def calculate_ADE(pred_x, pred_y, test_x, test_y):\n",
    "    total_displacement_error = 0\n",
    "    for point_idx in range(len(test_x)):\n",
    "        displacement_error = np.sqrt((pred_x[point_idx] - test_x[point_idx])**2 + (pred_y[point_idx] - test_y[point_idx])**2)\n",
    "        total_displacement_error += displacement_error\n",
    "\n",
    "    return total_displacement_error/len(pred_x)\n",
    "\n",
    "## The evaluation logic for Trajectron++ loops over the frames and predicts the future trajectories \n",
    "## for each node present in the current frame\n",
    "## Each node has to have at least 7 historical points and 12 future points\n",
    "def get_total_predictable_slices(data):\n",
    "    total_predictable_steps = 0\n",
    "    for i in pd.unique(data.node_id):\n",
    "        #print(len(test[test.node_id == i]))\n",
    "        total_predictable_steps += len(data[data.node_id == i]) - 19\n",
    "    return total_predictable_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907d43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_data):\n",
    "    data = input_data.copy()\n",
    "    data['frame_id'] = pd.to_numeric(data['frame_id'], downcast='integer')\n",
    "    data['track_id'] = pd.to_numeric(data['track_id'], downcast='integer')\n",
    "\n",
    "    data['frame_id'] = data['frame_id'] // 10\n",
    "\n",
    "    data['frame_id'] -= data['frame_id'].min()\n",
    "\n",
    "    data['node_type'] = 'PEDESTRIAN'\n",
    "    data['node_id'] = data['track_id'].astype(str)\n",
    "    data.sort_values('frame_id', inplace=True)\n",
    "\n",
    "    data['pos_x'] = data['pos_x'] - data['pos_x'].mean()\n",
    "    data['pos_y'] = data['pos_y'] - data['pos_y'].mean()\n",
    "    \n",
    "    # Select only such nodes which have enough data to predict on (7 historical timesteps, 12 future)\n",
    "    v = data.node_id.value_counts()\n",
    "    data = data[data.node_id.isin(v.index[v.gt(19)])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce140413",
   "metadata": {},
   "source": [
    "## Method evaluation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46e27d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_our_method(data, params, dataset_title='', single_output = False):\n",
    "    tot = 0\n",
    "    our_fde_best_of = []\n",
    "    our_ade_best_of = []\n",
    "\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='Ours - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        #print(frame_data)\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check if at least 7 historical points are present\n",
    "            # PS: It might be so that the prediction starts at the 8th step instead of 7th? Edited the code to do this at the moment\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Not sure why there has to be more than 12 frames to the future (at least 13) but it's the\n",
    "                # only way to get the number of trajectron++ eval predictions to match up\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    tot += 1\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    all_pred_x, all_pred_y, _ = generative_model.predict(x_data, y_data, params, trajectory_length=12)\n",
    "\n",
    "                    best_fde = None\n",
    "                    best_ade = None\n",
    "                    \n",
    "                    if single_output:\n",
    "                        avg_x = np.mean(all_pred_x, axis=0)\n",
    "                        avg_y = np.mean(all_pred_y, axis=0)\n",
    "                        best_fde = calculate_FDE(avg_x, avg_y, x_gt, y_gt)\n",
    "                        best_ade = calculate_ADE(avg_x, avg_y, x_gt, y_gt)\n",
    "                    else:\n",
    "                        for i in range(len(all_pred_x)):\n",
    "                            current_pred_x = all_pred_x[i]\n",
    "                            current_pred_y = all_pred_y[i]\n",
    "\n",
    "                            fde = calculate_FDE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                            if best_fde == None or fde < best_fde:\n",
    "                                best_fde = fde\n",
    "\n",
    "                            ade = calculate_ADE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                            if best_ade == None or ade < best_ade:\n",
    "                                best_ade = ade\n",
    "\n",
    "                    our_fde_best_of.append(best_fde)\n",
    "                    our_ade_best_of.append(best_ade)\n",
    "                    \n",
    "    return our_fde_best_of, our_ade_best_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "781a48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cvm_with_scenarios(data, dataset_title='', history_length=8):\n",
    "    tot = 0\n",
    "    our_fde_best_of = []\n",
    "    our_ade_best_of = []\n",
    "\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='CVM - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        #print(frame_data)\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check if at least 7 historical points are present\n",
    "            # PS: It might be so that the prediction starts at the 8th step instead of 7th? Edited the code to do this at the moment\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Not sure why there has to be more than 12 frames to the future (at least 13) but it's the\n",
    "                # only way to get the number of trajectron++ eval predictions to match up\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    tot += 1\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    all_pred_x, all_pred_y = [], []\n",
    "                    \n",
    "                    # CVM\n",
    "                    for i in range(20):\n",
    "                        history_x = x_data[-history_length:]\n",
    "                        history_y = y_data[-history_length:]\n",
    "                        assert len(history_x) == history_length\n",
    "                        \n",
    "                        if i == 0:\n",
    "                            vel_x = [history_x[i] - history_x[i-1] for i in range(1, len(history_x))]\n",
    "                            vel_y = [history_y[i] - history_y[i-1] for i in range(1, len(history_y))]\n",
    "                        else:\n",
    "                            vel_x = [(history_x[i] - history_x[i-1]) + np.random.normal(0, 1) for i in range(1, len(history_x))]\n",
    "                            vel_y = [(history_y[i] - history_y[i-1]) + np.random.normal(0, 1) for i in range(1, len(history_y))]\n",
    "                        \n",
    "                        assert len(vel_x) == history_length-1\n",
    "                        avg_vel_x = np.mean(vel_x)\n",
    "                        avg_vel_y = np.mean(vel_y)\n",
    "                        \n",
    "                        pred_x = [x_data[-1] + i*avg_vel_x for i in range(1, 13)]\n",
    "                        pred_y = [y_data[-1] + i*avg_vel_y for i in range(1, 13)]\n",
    "                        assert len(pred_x) == 12\n",
    "                        \n",
    "                        all_pred_x.append(pred_x)\n",
    "                        all_pred_y.append(pred_y)\n",
    "                    \n",
    "\n",
    "                    best_fde = None\n",
    "                    best_ade = None\n",
    "                    for i in range(len(all_pred_x)):\n",
    "                        current_pred_x = all_pred_x[i]\n",
    "                        current_pred_y = all_pred_y[i]\n",
    "\n",
    "                        fde = calculate_FDE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_fde == None or fde < best_fde:\n",
    "                            best_fde = fde\n",
    "\n",
    "                        ade = calculate_ADE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_ade == None or ade < best_ade:\n",
    "                            best_ade = ade\n",
    "\n",
    "                    our_fde_best_of.append(best_fde)\n",
    "                    our_ade_best_of.append(best_ade)\n",
    "                    \n",
    "    return our_fde_best_of, our_ade_best_of\n",
    "\n",
    "def evaluate_cvm(data, dataset_title='', history_length=8):\n",
    "    tot = 0\n",
    "    all_fde = []\n",
    "    all_ade = []\n",
    "\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='CVM - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        #print(frame_data)\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check if at least 7 historical points are present\n",
    "            # PS: It might be so that the prediction starts at the 8th step instead of 7th? Edited the code to do this at the moment\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Not sure why there has to be more than 12 frames to the future (at least 13) but it's the\n",
    "                # only way to get the number of trajectron++ eval predictions to match up\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    tot += 1\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    history_x = x_data[-history_length:]\n",
    "                    history_y = y_data[-history_length:]\n",
    "                    assert len(history_x) == history_length\n",
    "\n",
    "                    vel_x = [history_x[i] - history_x[i-1] for i in range(1, len(history_x))]\n",
    "                    vel_y = [history_y[i] - history_y[i-1] for i in range(1, len(history_y))]\n",
    "\n",
    "                    assert len(vel_x) == history_length-1\n",
    "                    avg_vel_x = np.mean(vel_x)\n",
    "                    avg_vel_y = np.mean(vel_y)\n",
    "\n",
    "                    pred_x = [x_data[-1] + i*avg_vel_x for i in range(1, 13)]\n",
    "                    pred_y = [y_data[-1] + i*avg_vel_y for i in range(1, 13)]\n",
    "                    assert len(pred_x) == 12\n",
    "\n",
    "                    fde = calculate_FDE(pred_x, pred_y, x_gt, y_gt)\n",
    "                    ade = calculate_ADE(pred_x, pred_y, x_gt, y_gt)\n",
    "\n",
    "                    all_fde.append(fde)\n",
    "                    all_ade.append(ade)\n",
    "                    \n",
    "    return all_fde, all_ade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a687f69",
   "metadata": {},
   "source": [
    "## Automated results comparison - Best of 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef2ab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trajectron_data(trajectron_resultset_name, suffix='best_of'):\n",
    "    trajectron_fde = []\n",
    "    with open('pedestrians/results/' + trajectron_resultset_name + '_fde_' + suffix + '.csv', mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            trajectron_fde.append(float(row['value']))\n",
    "\n",
    "    trajectron_ade = []\n",
    "    with open('pedestrians/results/' + trajectron_resultset_name + '_ade_'+ suffix + '.csv', mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            trajectron_ade.append(float(row['value']))\n",
    "            \n",
    "    return trajectron_fde, trajectron_ade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cf14c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_datasets(our_method_params, files, trajectron_resultset_names, trajectron_ar3_resultset_names, evaluate_most_likely=False):\n",
    "    base_path = './pedestrians/raw/'\n",
    "\n",
    "    ours_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    trajectron_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    trajectron_ar3_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    cvm_long_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    cvm_short_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "\n",
    "    for file_idx, file in enumerate(files):\n",
    "        data = pd.read_csv(base_path + file, sep='\\t', index_col=False, header=None)\n",
    "        data.columns = ['frame_id', 'track_id', 'pos_x', 'pos_y']\n",
    "\n",
    "        data = process_data(data)\n",
    "\n",
    "        ## Trajectron\n",
    "        trajectron_fde, trajectron_ade = read_trajectron_data(trajectron_resultset_names[file_idx])\n",
    "        trajectron_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_fde))\n",
    "        trajectron_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ade))\n",
    "        \n",
    "        if evaluate_most_likely:\n",
    "            trajectron_fde, trajectron_ade = read_trajectron_data(trajectron_resultset_names[file_idx], suffix='most_likely')\n",
    "            trajectron_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_fde))\n",
    "            trajectron_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ade))\n",
    "        \n",
    "        ## Trajectron with interactions\n",
    "        trajectron_ar3_fde, trajectron_ar3_ade = read_trajectron_data(trajectron_ar3_resultset_names[file_idx])\n",
    "        trajectron_ar3_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_ar3_fde))\n",
    "        trajectron_ar3_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ar3_ade))\n",
    "        \n",
    "        if evaluate_most_likely:\n",
    "            trajectron_ar3_fde, trajectron_ar3_ade = read_trajectron_data(trajectron_resultset_names[file_idx], suffix='most_likely')\n",
    "            trajectron_ar3_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_ar3_fde))\n",
    "            trajectron_ar3_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ar3_ade))\n",
    "\n",
    "        # make sure that there is no discrepancy between our data processing and trajectron evaluation results size\n",
    "        num_predictable_trajectories = get_total_predictable_slices(data)\n",
    "        assert len(trajectron_fde) == num_predictable_trajectories\n",
    "        assert len(trajectron_ade) == num_predictable_trajectories\n",
    "\n",
    "        ## Ours\n",
    "        our_fde_best_of_20, our_ade_best_of_20 = evaluate_our_method(data, our_method_params[file_idx], dataset_title=trajectron_resultset_names[file_idx])\n",
    "        ours_results['BEST_OF_20']['FDE'].append(np.mean(our_fde_best_of_20))\n",
    "        ours_results['BEST_OF_20']['ADE'].append(np.mean(our_ade_best_of_20))\n",
    "        \n",
    "        if evaluate_most_likely:\n",
    "            our_fde_single, our_ade_single = evaluate_our_method(data, our_method_params[file_idx], dataset_title=trajectron_resultset_names[file_idx], single_output=True)\n",
    "            ours_results['MOST_LIKELY']['FDE'].append(np.mean(our_fde_single))\n",
    "            ours_results['MOST_LIKELY']['ADE'].append(np.mean(our_ade_single))\n",
    "\n",
    "        ## CVM\n",
    "        cvm_fde_best_of, cvm_ade_best_of = evaluate_cvm_with_scenarios(data, dataset_title=trajectron_resultset_names[file_idx])\n",
    "        cvm_fde_short_history_best_of, cvm_ade_short_history_best_of = evaluate_cvm_with_scenarios(data, dataset_title=trajectron_resultset_names[file_idx], history_length=2)\n",
    "        cvm_long_results['BEST_OF_20']['FDE'].append(np.mean(cvm_fde_best_of))\n",
    "        cvm_long_results['BEST_OF_20']['ADE'].append(np.mean(cvm_ade_best_of))\n",
    "        cvm_short_results['BEST_OF_20']['FDE'].append(np.mean(cvm_fde_short_history_best_of))\n",
    "        cvm_short_results['BEST_OF_20']['ADE'].append(np.mean(cvm_ade_short_history_best_of))\n",
    "        \n",
    "        if evaluate_most_likely:\n",
    "            cvm_fde, cvm_ade = evaluate_cvm(data, dataset_title=trajectron_resultset_names[file_idx])\n",
    "            cvm_fde_short_history, cvm_ade_short_history = evaluate_cvm(data, dataset_title=trajectron_resultset_names[file_idx], history_length=2)\n",
    "            cvm_long_results['MOST_LIKELY']['FDE'].append(np.mean(cvm_fde))\n",
    "            cvm_long_results['MOST_LIKELY']['ADE'].append(np.mean(cvm_ade))\n",
    "            cvm_short_results['MOST_LIKELY']['FDE'].append(np.mean(cvm_fde_short_history))\n",
    "            cvm_short_results['MOST_LIKELY']['ADE'].append(np.mean(cvm_ade_short_history))\n",
    "    \n",
    "    # Omit univ for now as it takes very long to evaluate\n",
    "    \n",
    "    ## Univ is a separate case as it has 2 scenes\n",
    "    filename1 = './pedestrians/raw/univ/test/students001.txt'\n",
    "    filename2 = './pedestrians/raw/univ/test/students003.txt'\n",
    "    univ_data_1 = pd.read_csv(filename1, sep='\\t', index_col=False, header=None)\n",
    "    univ_data_2 = pd.read_csv(filename2, sep='\\t', index_col=False, header=None)\n",
    "    univ_data_1.columns = ['frame_id', 'track_id', 'pos_x', 'pos_y']\n",
    "    univ_data_2.columns = ['frame_id', 'track_id', 'pos_x', 'pos_y']\n",
    "\n",
    "    univ_data_1 = process_data(univ_data_1)\n",
    "    univ_data_2 = process_data(univ_data_2)\n",
    "\n",
    "    ## Ours\n",
    "    our_fde_best_of_1, our_ade_best_of_1 = evaluate_our_method(univ_data_1, our_method_params[-1], dataset_title='univ 1')\n",
    "    our_fde_best_of_2, our_ade_best_of_2 = evaluate_our_method(univ_data_2, our_method_params[-1], dataset_title='univ 2')\n",
    "\n",
    "    our_fde_best_of_20 = our_fde_best_of_1 + our_fde_best_of_2\n",
    "    our_ade_best_of_20 = our_ade_best_of_1 + our_ade_best_of_2\n",
    "\n",
    "    ours_results['BEST_OF_20']['FDE'].append(np.mean(our_fde_best_of_20))\n",
    "    ours_results['BEST_OF_20']['ADE'].append(np.mean(our_ade_best_of_20))\n",
    "    \n",
    "    if evaluate_most_likely:\n",
    "        our_fde_ml_1, our_ade_ml_1 = evaluate_our_method(univ_data_1, our_method_params[-1], dataset_title='univ 1', single_output=True)\n",
    "        our_fde_ml_2, our_ade_ml_2 = evaluate_our_method(univ_data_2, our_method_params[-1], dataset_title='univ 2', single_output=True)\n",
    "\n",
    "        our_fde_ml = our_fde_ml_1 + our_fde_ml_2\n",
    "        our_ade_ml = our_ade_ml_1 + our_ade_ml_2\n",
    "\n",
    "        ours_results['MOST_LIKELY']['FDE'].append(np.mean(our_fde_ml))\n",
    "        ours_results['MOST_LIKELY']['ADE'].append(np.mean(our_ade_ml))\n",
    "\n",
    "    ## CVM\n",
    "    \n",
    "    cvm_fde_best_of_1, cvm_ade_best_of_1 = evaluate_cvm_with_scenarios(univ_data_1, dataset_title='univ 1')\n",
    "    cvm_fde_best_of_2, cvm_ade_best_of_2 = evaluate_cvm_with_scenarios(univ_data_2, dataset_title='univ 2')\n",
    "\n",
    "    cvm_fde_best_of = cvm_fde_best_of_1 + cvm_fde_best_of_2\n",
    "    cvm_ade_best_of = cvm_ade_best_of_1 + cvm_ade_best_of_2\n",
    "\n",
    "    cvm_fde_short_hist_best_of_1, cvm_ade_short_hist_best_of_1 = evaluate_cvm_with_scenarios(univ_data_1, dataset_title='univ 1', history_length=2)\n",
    "    cvm_fde_short_hist_best_of_2, cvm_ade_short_hist_best_of_2 = evaluate_cvm_with_scenarios(univ_data_2, dataset_title='univ 2', history_length=2)\n",
    "\n",
    "    cvm_fde_short_history_best_of = cvm_fde_short_hist_best_of_1 + cvm_fde_short_hist_best_of_2\n",
    "    cvm_ade_short_history_best_of = cvm_ade_short_hist_best_of_1 + cvm_ade_short_hist_best_of_2\n",
    "\n",
    "    cvm_long_results['BEST_OF_20']['FDE'].append(np.mean(cvm_fde_best_of))\n",
    "    cvm_long_results['BEST_OF_20']['ADE'].append(np.mean(cvm_ade_best_of))\n",
    "    cvm_short_results['BEST_OF_20']['FDE'].append(np.mean(cvm_fde_short_history_best_of))\n",
    "    cvm_short_results['BEST_OF_20']['ADE'].append(np.mean(cvm_ade_short_history_best_of))\n",
    "    \n",
    "    if evaluate_most_likely:\n",
    "        cvm_fde_ml_1, cvm_ade_ml_1 = evaluate_cvm(univ_data_1, dataset_title='univ 1')\n",
    "        cvm_fde_ml_2, cvm_ade_ml_2 = evaluate_cvm(univ_data_2, dataset_title='univ 2')\n",
    "\n",
    "        cvm_fde_ml = cvm_fde_ml_1 + cvm_fde_ml_2\n",
    "        cvm_ade_ml = cvm_ade_ml_1 + cvm_ade_ml_2\n",
    "\n",
    "        cvm_fde_short_hist_ml_1, cvm_ade_short_hist_ml_1 = evaluate_cvm(univ_data_1, dataset_title='univ 1', history_length=2)\n",
    "        cvm_fde_short_hist_ml_2, cvm_ade_short_hist_ml_2 = evaluate_cvm(univ_data_2, dataset_title='univ 2', history_length=2)\n",
    "\n",
    "        cvm_fde_short_history_ml = cvm_fde_short_hist_ml_1 + cvm_ade_short_hist_ml_1\n",
    "        cvm_ade_short_history_ml = cvm_ade_short_hist_ml_1 + cvm_ade_short_hist_ml_2\n",
    "\n",
    "        cvm_long_results['MOST_LIKELY']['FDE'].append(np.mean(cvm_fde_ml))\n",
    "        cvm_long_results['MOST_LIKELY']['ADE'].append(np.mean(cvm_ade_ml))\n",
    "        cvm_short_results['MOST_LIKELY']['FDE'].append(np.mean(cvm_fde_short_history_ml))\n",
    "        cvm_short_results['MOST_LIKELY']['ADE'].append(np.mean(cvm_ade_short_history_ml))  \n",
    "        \n",
    "    ## Trajectron\n",
    "    trajectron_fde, trajectron_ade = read_trajectron_data('univ_vel')\n",
    "    trajectron_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_fde))\n",
    "    trajectron_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ade))\n",
    "    \n",
    "    if evaluate_most_likely:\n",
    "        trajectron_fde, trajectron_ade = read_trajectron_data('univ_vel', suffix='most_likely')\n",
    "        trajectron_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_fde))\n",
    "        trajectron_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ade))\n",
    "    \n",
    "    ## Trajectron with interactions\n",
    "    trajectron_ar3_fde, trajectron_ar3_ade = read_trajectron_data('univ_ar3')\n",
    "    trajectron_ar3_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_ar3_fde))\n",
    "    trajectron_ar3_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ar3_ade))\n",
    "    \n",
    "    if evaluate_most_likely:\n",
    "        trajectron_ar3_fde, trajectron_ar3_ade = read_trajectron_data('univ_ar3', suffix='most_likely')\n",
    "        trajectron_ar3_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_ar3_fde))\n",
    "        trajectron_ar3_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ar3_ade))\n",
    "    \n",
    "    \n",
    "    return [\n",
    "        ours_results,\n",
    "        trajectron_results,\n",
    "        trajectron_ar3_results,\n",
    "        cvm_long_results,\n",
    "        cvm_short_results\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ours - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 484/484 [01:03<00:00,  7.57it/s]\n",
      "Ours - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 484/484 [01:02<00:00,  7.69it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 484.78it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 559.51it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 745.57it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 747.02it/s]\n",
      "Ours - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 913/913 [03:20<00:00,  4.54it/s]\n",
      "Ours - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 913/913 [03:21<00:00,  4.53it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 913/913 [00:03<00:00, 250.70it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 913/913 [00:03<00:00, 279.39it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 913/913 [00:02<00:00, 355.29it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 913/913 [00:02<00:00, 355.72it/s]\n",
      "Ours - zara1_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 864/864 [32:11<00:00,  2.24s/it]\n",
      "Ours - zara1_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 864/864 [06:25<00:00,  2.24it/s]\n",
      "CVM - zara1_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 864/864 [00:06<00:00, 124.44it/s]\n",
      "CVM - zara1_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 864/864 [00:06<00:00, 139.29it/s]\n",
      "CVM - zara1_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 864/864 [00:04<00:00, 179.08it/s]\n",
      "CVM - zara1_vel: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 864/864 [00:04<00:00, 179.10it/s]\n",
      "Ours - zara2_vel: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1052/1052 [54:01<00:00,  3.08s/it]\n",
      "Ours - zara2_vel: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1052/1052 [16:09<00:00,  1.09it/s]\n",
      "CVM - zara2_vel: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1052/1052 [00:20<00:00, 52.06it/s]\n",
      "CVM - zara2_vel: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1052/1052 [00:18<00:00, 56.70it/s]\n",
      "CVM - zara2_vel: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1052/1052 [00:14<00:00, 71.19it/s]\n",
      "CVM - zara2_vel: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1052/1052 [00:14<00:00, 70.97it/s]\n",
      "Ours - univ 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 444/444 [41:11<00:00,  5.57s/it]\n",
      "Ours - univ 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 541/541 [29:13<00:00,  3.24s/it]\n",
      "Ours - univ 1:   7%|██████▍                                                                                        | 30/444 [02:51<44:24,  6.44s/it]"
     ]
    }
   ],
   "source": [
    "params_eth = {\n",
    "    \"NOISE\": 0.05,\n",
    "    \"NO_OF_TRAJECTORIES\": 1000,\n",
    "    \"CONST_VEL_MODEL_PROB\": 0.8,\n",
    "\n",
    "    \"STOP_PROB\": 0.05,\n",
    "\n",
    "    \"DISCOUNT_AVG_PROB\": 0.5,\n",
    "    \"DISCOUNT_LOWER_BOUND\": 0.5,\n",
    "\n",
    "    \"VELOCITY_CHANGE_PROB\": 0.2,\n",
    "    \"VELOCITY_CHANGE_NOISE\": 0.15,\n",
    "\n",
    "    \"ANGLE_CHANGE_PROB\": 0.2,\n",
    "    \"ANGLE_CHANGE_NOISE\": 1,\n",
    "\n",
    "    \"GROUP_PERCENTAGES\": [0.10, 0.35, 0.68, 0.95, 1.0],\n",
    "    \"GROUP_CLUSTER_COUNT\": [1, 6, 5, 5, 3] # Total 20 traj\n",
    "}\n",
    "\n",
    "params_rest = {\n",
    "    \"NOISE\": 0.05,\n",
    "    \"NO_OF_TRAJECTORIES\": 1000,\n",
    "    \"CONST_VEL_MODEL_PROB\": 0.9,\n",
    "\n",
    "    \"STOP_PROB\": 0.05,\n",
    "\n",
    "    \"DISCOUNT_AVG_PROB\": 0.75,\n",
    "    \"DISCOUNT_LOWER_BOUND\": 0.5,\n",
    "\n",
    "    \"VELOCITY_CHANGE_PROB\": 0.1,\n",
    "    \"VELOCITY_CHANGE_NOISE\": 0.1,\n",
    "\n",
    "    \"ANGLE_CHANGE_PROB\": 0.05,\n",
    "    \"ANGLE_CHANGE_NOISE\": 1,\n",
    "\n",
    "    \"GROUP_PERCENTAGES\": [0.10, 0.40, 0.65, 0.85, 1.0],\n",
    "    \"GROUP_CLUSTER_COUNT\": [1, 6, 5, 5, 3] # Total 20 traj\n",
    "}\n",
    "\n",
    "our_method_params = [params_eth, params_eth, params_rest, params_rest]\n",
    "\n",
    "files = [\n",
    "    'eth/test/biwi_eth.txt', \n",
    "    'hotel/test/biwi_hotel.txt', \n",
    "    'zara1/test/crowds_zara01.txt', \n",
    "    'zara2/test/crowds_zara02.txt'\n",
    "]\n",
    "\n",
    "trajectron_resultset_names = [\n",
    "    'eth_vel', \n",
    "    'hotel_vel', \n",
    "    'zara1_vel', \n",
    "    'zara2_vel'\n",
    "]\n",
    "\n",
    "trajectron_ar3_resultset_names = [\n",
    "    'eth_ar3', \n",
    "    'hotel_ar3', \n",
    "    'zara1_ar3', \n",
    "    'zara2_ar3'\n",
    "]\n",
    "\n",
    "res = evaluate_all_datasets(our_method_params, files, trajectron_resultset_names, trajectron_ar3_resultset_names, evaluate_most_likely=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bedb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ours_results = res[0]\n",
    "trajectron_results = res[1]\n",
    "trajectron_ar3_results = res[2]\n",
    "cvm_long_results = res[3]\n",
    "cvm_short_results = res[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab25de20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (5) does not match length of index (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m index \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mETH\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHotel\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#'Univ'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      9\u001b[0m df_data_best_of_20_fde \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCVM (8pt history)\u001b[39m\u001b[38;5;124m'\u001b[39m : pd\u001b[38;5;241m.\u001b[39mSeries(cvm_long_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEST_OF_20\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFDE\u001b[39m\u001b[38;5;124m'\u001b[39m], index \u001b[38;5;241m=\u001b[39m index),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCVM (2pt history)\u001b[39m\u001b[38;5;124m'\u001b[39m : pd\u001b[38;5;241m.\u001b[39mSeries(cvm_short_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEST_OF_20\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFDE\u001b[39m\u001b[38;5;124m'\u001b[39m], index \u001b[38;5;241m=\u001b[39m index),\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrajectron++\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectron_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBEST_OF_20\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFDE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrajectron++ AR3\u001b[39m\u001b[38;5;124m'\u001b[39m : pd\u001b[38;5;241m.\u001b[39mSeries(trajectron_ar3_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEST_OF_20\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFDE\u001b[39m\u001b[38;5;124m'\u001b[39m], index \u001b[38;5;241m=\u001b[39m index),\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOurs\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mSeries(ours_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEST_OF_20\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFDE\u001b[39m\u001b[38;5;124m'\u001b[39m], index \u001b[38;5;241m=\u001b[39m index)\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m df_best_of_20_fde \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_data_best_of_20_fde)\n\u001b[1;32m     19\u001b[0m df_data_best_of_20_ade \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCVM (8pt history)\u001b[39m\u001b[38;5;124m'\u001b[39m : pd\u001b[38;5;241m.\u001b[39mSeries(cvm_long_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEST_OF_20\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADE\u001b[39m\u001b[38;5;124m'\u001b[39m], index \u001b[38;5;241m=\u001b[39m index),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCVM (2pt history)\u001b[39m\u001b[38;5;124m'\u001b[39m : pd\u001b[38;5;241m.\u001b[39mSeries(cvm_short_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEST_OF_20\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADE\u001b[39m\u001b[38;5;124m'\u001b[39m], index \u001b[38;5;241m=\u001b[39m index),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOurs\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mSeries(ours_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEST_OF_20\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADE\u001b[39m\u001b[38;5;124m'\u001b[39m], index \u001b[38;5;241m=\u001b[39m index)\n\u001b[1;32m     25\u001b[0m }\n",
      "File \u001b[0;32m~/miniforge3/envs/trajectron++/lib/python3.8/site-packages/pandas/core/series.py:442\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    440\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[0;32m--> 442\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "File \u001b[0;32m~/miniforge3/envs/trajectron++/lib/python3.8/site-packages/pandas/core/common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (5) does not match length of index (4)"
     ]
    }
   ],
   "source": [
    "index = [\n",
    "    'ETH', \n",
    "    'Hotel', \n",
    "    'Zara 1', \n",
    "    'Zara 2',\n",
    "    'Univ'\n",
    "]\n",
    "\n",
    "df_data_best_of_20_fde = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['BEST_OF_20']['FDE'], index = index)\n",
    "}\n",
    "\n",
    "df_best_of_20_fde = pd.DataFrame(df_data_best_of_20_fde)\n",
    "\n",
    "df_data_best_of_20_ade = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['BEST_OF_20']['ADE'], index = index)\n",
    "}\n",
    "\n",
    "df_best_of_20_ade = pd.DataFrame(df_data_best_of_20_ade)\n",
    "\n",
    "df_data_most_likely_fde = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['MOST_LIKELY']['FDE'], index = index)\n",
    "}\n",
    "\n",
    "df_most_likely_fde = pd.DataFrame(df_data_most_likely_fde)\n",
    "\n",
    "df_data_most_likely_ade = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['MOST_LIKELY']['ADE'], index = index)\n",
    "}\n",
    "\n",
    "df_most_likely_ade = pd.DataFrame(df_data_most_likely_ade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89de3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fde\n",
    "df_data_best_of_20_fde.style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_of_20_ade.style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_likely_fde.style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_likely_ade.style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf99d3",
   "metadata": {},
   "source": [
    "## Trying many sets of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cddbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    \"NOISE\": 0.05,\n",
    "    \"NO_OF_TRAJECTORIES\": 1000,\n",
    "    \"CONST_VEL_MODEL_PROB\": 0.8,\n",
    "\n",
    "    \"STOP_PROB\": 0.05,\n",
    "\n",
    "    \"DISCOUNT_AVG_PROB\": 0.5,\n",
    "    \"DISCOUNT_LOWER_BOUND\": 0.5,\n",
    "\n",
    "    \"VELOCITY_CHANGE_PROB\": 0.2,\n",
    "    \"VELOCITY_CHANGE_NOISE\": 0.15,\n",
    "\n",
    "    \"ANGLE_CHANGE_PROB\": 0.2,\n",
    "    \"ANGLE_CHANGE_NOISE\": 1,\n",
    "\n",
    "    \"GROUP_PERCENTAGES\": [0.10, 0.35, 0.68, 0.95, 1.0],\n",
    "    \"GROUP_CLUSTER_COUNT\": [1, 6, 5, 5, 3] # Total 20 traj\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    \"NOISE\": 0.05,\n",
    "    \"NO_OF_TRAJECTORIES\": 1000,\n",
    "    \"CONST_VEL_MODEL_PROB\": 0.9,\n",
    "\n",
    "    \"STOP_PROB\": 0.05,\n",
    "\n",
    "    \"DISCOUNT_AVG_PROB\": 0.75,\n",
    "    \"DISCOUNT_LOWER_BOUND\": 0.5,\n",
    "\n",
    "    \"VELOCITY_CHANGE_PROB\": 0.1,\n",
    "    \"VELOCITY_CHANGE_NOISE\": 0.1,\n",
    "\n",
    "    \"ANGLE_CHANGE_PROB\": 0.05,\n",
    "    \"ANGLE_CHANGE_NOISE\": 1,\n",
    "\n",
    "    \"GROUP_PERCENTAGES\": [0.10, 0.40, 0.65, 0.85, 1.0],\n",
    "    \"GROUP_CLUSTER_COUNT\": [1, 6, 5, 5, 3] # Total 20 traj\n",
    "}\n",
    "\n",
    "params3 = {\n",
    "    \"NOISE\": 0.05,\n",
    "    \"NO_OF_TRAJECTORIES\": 1000,\n",
    "    \"CONST_VEL_MODEL_PROB\": 0.8,\n",
    "\n",
    "    \"STOP_PROB\": 0.1,\n",
    "\n",
    "    \"DISCOUNT_AVG_PROB\": 0.3,\n",
    "    \"DISCOUNT_LOWER_BOUND\": 0.5,\n",
    "\n",
    "    \"VELOCITY_CHANGE_PROB\": 0.2,\n",
    "    \"VELOCITY_CHANGE_NOISE\": 0.15,\n",
    "\n",
    "    \"ANGLE_CHANGE_PROB\": 0.2,\n",
    "    \"ANGLE_CHANGE_NOISE\": 1,\n",
    "\n",
    "    \"GROUP_PERCENTAGES\": [0.10, 0.35, 0.68, 0.95, 1.0],\n",
    "    \"GROUP_CLUSTER_COUNT\": [1, 6, 5, 5, 3] # Total 20 traj\n",
    "}\n",
    "\n",
    "params4 = {\n",
    "    \"NOISE\": 0.05,\n",
    "    \"NO_OF_TRAJECTORIES\": 1000,\n",
    "    \"CONST_VEL_MODEL_PROB\": 0.7,\n",
    "\n",
    "    \"STOP_PROB\": 0.05,\n",
    "\n",
    "    \"DISCOUNT_AVG_PROB\": 0.5,\n",
    "    \"DISCOUNT_LOWER_BOUND\": 0.5,\n",
    "\n",
    "    \"VELOCITY_CHANGE_PROB\": 0.1,\n",
    "    \"VELOCITY_CHANGE_NOISE\": 0.1,\n",
    "\n",
    "    \"ANGLE_CHANGE_PROB\": 0.05,\n",
    "    \"ANGLE_CHANGE_NOISE\": 1,\n",
    "\n",
    "    \"GROUP_PERCENTAGES\": [0.10, 0.4, 0.7, 1.0],\n",
    "    \"GROUP_CLUSTER_COUNT\": [1, 7, 7, 5] # Total 20 traj\n",
    "}\n",
    "\n",
    "params5 = {\n",
    "    \"NOISE\": 0.05,\n",
    "    \"NO_OF_TRAJECTORIES\": 1000,\n",
    "    \"CONST_VEL_MODEL_PROB\": 0.7,\n",
    "\n",
    "    \"STOP_PROB\": 0.05,\n",
    "\n",
    "    \"DISCOUNT_AVG_PROB\": 0.3,\n",
    "    \"DISCOUNT_LOWER_BOUND\": 0.5,\n",
    "\n",
    "    \"VELOCITY_CHANGE_PROB\": 0.1,\n",
    "    \"VELOCITY_CHANGE_NOISE\": 0.1,\n",
    "\n",
    "    \"ANGLE_CHANGE_PROB\": 0.025,\n",
    "    \"ANGLE_CHANGE_NOISE\": 1,\n",
    "\n",
    "    \"GROUP_PERCENTAGES\": [0.10, 0.4, 0.7, 1.0],\n",
    "    \"GROUP_CLUSTER_COUNT\": [1, 7, 7, 5] # Total 20 traj\n",
    "}\n",
    "\n",
    "params6 = {\n",
    "    \"NOISE\": 0.05,\n",
    "    \"NO_OF_TRAJECTORIES\": 2000,\n",
    "    \"CONST_VEL_MODEL_PROB\": 0.7,\n",
    "\n",
    "    \"STOP_PROB\": 0.05,\n",
    "\n",
    "    \"DISCOUNT_AVG_PROB\": 0.3,\n",
    "    \"DISCOUNT_LOWER_BOUND\": 0.5,\n",
    "\n",
    "    \"VELOCITY_CHANGE_PROB\": 0.1,\n",
    "    \"VELOCITY_CHANGE_NOISE\": 0.1,\n",
    "\n",
    "    \"ANGLE_CHANGE_PROB\": 0.025,\n",
    "    \"ANGLE_CHANGE_NOISE\": 1,\n",
    "\n",
    "    \"GROUP_PERCENTAGES\": [0.05, 0.5, 0.8, 1.0],\n",
    "    \"GROUP_CLUSTER_COUNT\": [1, 8, 6, 5] # Total 20 traj\n",
    "}\n",
    "\n",
    "param_set = [params1, params2, params3, params4, params5, params6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_bo20_fde = []\n",
    "all_df_bo20_ade = []\n",
    "all_df_ml_fde = []\n",
    "all_df_ml_ade = []\n",
    "for idx, params in enumerate(param_set):\n",
    "    our_method_params = [params, params, params, params]\n",
    "    res = evaluate_all_datasets(our_method_params, files, trajectron_resultset_names, trajectron_ar3_resultset_names, evaluate_most_likely=True)\n",
    "    \n",
    "    ours_results = res[0]\n",
    "    trajectron_results = res[1]\n",
    "    trajectron_ar3_results = res[2]\n",
    "    cvm_long_results = res[3]\n",
    "    cvm_short_results = res[4]\n",
    "\n",
    "    index = [\n",
    "    'ETH', \n",
    "    'Hotel', \n",
    "    'Zara 1', \n",
    "    'Zara 2',\n",
    "    'Univ'\n",
    "    ]\n",
    "\n",
    "    df_data_best_of_20_fde = {\n",
    "        'CVM (8pt history)' : pd.Series(cvm_long_results['BEST_OF_20']['FDE'], index = index),\n",
    "        'CVM (2pt history)' : pd.Series(cvm_short_results['BEST_OF_20']['FDE'], index = index),\n",
    "        'Trajectron++' : pd.Series(trajectron_results['BEST_OF_20']['FDE'], index = index),\n",
    "        'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['BEST_OF_20']['FDE'], index = index),\n",
    "        'Ours': pd.Series(ours_results['BEST_OF_20']['FDE'], index = index)\n",
    "    }\n",
    "\n",
    "    df_best_of_20_fde = pd.DataFrame(df_data_best_of_20_fde)\n",
    "    all_df_bo20_fde.append(df_best_of_20_fde)\n",
    "\n",
    "    df_data_best_of_20_ade = {\n",
    "        'CVM (8pt history)' : pd.Series(cvm_long_results['BEST_OF_20']['ADE'], index = index),\n",
    "        'CVM (2pt history)' : pd.Series(cvm_short_results['BEST_OF_20']['ADE'], index = index),\n",
    "        'Trajectron++' : pd.Series(trajectron_results['BEST_OF_20']['ADE'], index = index),\n",
    "        'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['BEST_OF_20']['ADE'], index = index),\n",
    "        'Ours': pd.Series(ours_results['BEST_OF_20']['ADE'], index = index)\n",
    "    }\n",
    "\n",
    "    df_best_of_20_ade = pd.DataFrame(df_data_best_of_20_ade)\n",
    "    all_df_bo20_ade.append(df_best_of_20_ade)\n",
    "\n",
    "    df_data_most_likely_fde = {\n",
    "        'CVM (8pt history)' : pd.Series(cvm_long_results['MOST_LIKELY']['FDE'], index = index),\n",
    "        'CVM (2pt history)' : pd.Series(cvm_short_results['MOST_LIKELY']['FDE'], index = index),\n",
    "        'Trajectron++' : pd.Series(trajectron_results['MOST_LIKELY']['FDE'], index = index),\n",
    "        'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['MOST_LIKELY']['FDE'], index = index),\n",
    "        'Ours': pd.Series(ours_results['MOST_LIKELY']['FDE'], index = index)\n",
    "    }\n",
    "\n",
    "    df_most_likely_fde = pd.DataFrame(df_data_most_likely_fde)\n",
    "    all_df_ml_fde.append(df_most_likely_fde)\n",
    "\n",
    "    df_data_most_likely_ade = {\n",
    "        'CVM (8pt history)' : pd.Series(cvm_long_results['MOST_LIKELY']['ADE'], index = index),\n",
    "        'CVM (2pt history)' : pd.Series(cvm_short_results['MOST_LIKELY']['ADE'], index = index),\n",
    "        'Trajectron++' : pd.Series(trajectron_results['MOST_LIKELY']['ADE'], index = index),\n",
    "        'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['MOST_LIKELY']['ADE'], index = index),\n",
    "        'Ours': pd.Series(ours_results['MOST_LIKELY']['ADE'], index = index)\n",
    "    }\n",
    "\n",
    "    df_most_likely_ade = pd.DataFrame(df_data_most_likely_ade)\n",
    "    all_df_ml_ade.append(df_most_likely_ade)\n",
    "    \n",
    "    print(idx)\n",
    "    print(df_fde)\n",
    "    print(df_ade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd1fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
