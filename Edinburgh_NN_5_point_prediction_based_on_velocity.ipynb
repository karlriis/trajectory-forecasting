{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06891f97",
   "metadata": {},
   "source": [
    "## Neural network predicting 5 points at once based on velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43186641",
   "metadata": {
    "id": "43186641"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f124715",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7407d78",
   "metadata": {
    "id": "a7407d78"
   },
   "outputs": [],
   "source": [
    "from data_processing import read_edinburgh_data \n",
    "# set sampling rate as 9 so that the velocities in the df are correct\n",
    "data, agent_ids = read_edinburgh_data(sampling_rate=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7210a",
   "metadata": {
    "id": "f7c7210a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_agent_ids, test_agent_ids = train_test_split(agent_ids, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69be78",
   "metadata": {
    "id": "7a69be78"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ee314",
   "metadata": {
    "id": "987ee314"
   },
   "outputs": [],
   "source": [
    "def create_dataset(data, agent_ids):\n",
    "    data_X, data_Y_vel_x, data_Y_vel_y = [], [], []\n",
    "    for agent_id in agent_ids:\n",
    "        # We only see 4 velocities, so train is 4 velocities long\n",
    "        x = data[data['agent_id'] == agent_id].iloc[1:5][['vel_x', 'vel_y']]\n",
    "        y_vel_x = data[data['agent_id'] == agent_id].iloc[5:10]['vel_x']\n",
    "        y_vel_y = data[data['agent_id'] == agent_id].iloc[5:10]['vel_y']\n",
    "        data_X.append(x)\n",
    "        data_Y_vel_x.append(y_vel_x)\n",
    "        data_Y_vel_y.append(y_vel_y)\n",
    "    return np.array(data_X), np.array(data_Y_vel_x), np.array(data_Y_vel_y)\n",
    "    \n",
    "def calculate_error(pred_x, pred_y, test_x, test_y):\n",
    "    \n",
    "    # FDE\n",
    "    final_displacement_x = pred_x[-1] - test_x[-1]\n",
    "    final_displacement_y = pred_y[-1] - test_y[-1]\n",
    "    FDE = np.sqrt(final_displacement_x**2 + final_displacement_y**2)\n",
    "    \n",
    "    # MSE\n",
    "    abs_error_x = pred_x - test_x\n",
    "    abs_error_y = pred_y - test_y\n",
    "    \n",
    "    MSE = np.mean(np.square(abs_error_x)) + np.mean(np.square(abs_error_y))\n",
    "    \n",
    "    return FDE, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c085ad",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fa56d",
   "metadata": {
    "id": "7f3fa56d"
   },
   "outputs": [],
   "source": [
    "train = create_dataset(data, train_agent_ids)\n",
    "test = create_dataset(data, test_agent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc388252",
   "metadata": {
    "id": "fc388252"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "\n",
    "# Note: RMSE seems to yield a worse loss than mean_squared_error\n",
    "def RMSE(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    \n",
    "def RMSE_with_discount(y_true, y_pred):\n",
    "    difference = y_true - y_pred\n",
    "    difference = difference * [1, 0.95, 0.90, 0.85, 0.80]\n",
    "    return K.sqrt(K.mean(K.square(difference))) \n",
    "    \n",
    "\n",
    "#inp = Input((5,2))\n",
    "inp = Input((4,2))\n",
    "\n",
    "x = Flatten()(inp)\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "\n",
    "'''\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "'''\n",
    "out1 = Dense(5, activation='linear')(x)    \n",
    "out2 = Dense(5, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=[out1,out2])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss = RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad3d42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2ad3d42",
    "outputId": "83b641d4-9d00-4d8f-a643-c4a48ddaaa46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_vel_x, y_vel_y = train[1], train[2] # separate the y data into x and y position columns\n",
    "model.fit(train[0], [y_vel_x, y_vel_y], epochs=400, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BqPlv1LBOx7C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqPlv1LBOx7C",
    "outputId": "81501cf3-4d2c-4958-a105-fb52f5b8c1f5"
   },
   "outputs": [],
   "source": [
    "trainScore = model.evaluate(train[0], [train[1], train[2]], verbose=0)\n",
    "print('Train Score: %.2f' % (trainScore[0]))\n",
    "\n",
    "testScore = model.evaluate(test[0], [test[1], test[2]], verbose=0)\n",
    "print('Test Score: %.2f' % (testScore[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b77d2",
   "metadata": {},
   "source": [
    "## Making predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wdLHm-yCNbJU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wdLHm-yCNbJU",
    "outputId": "19757ef5-3529-4b8e-b5b8-1f7b43deff58"
   },
   "outputs": [],
   "source": [
    "all_FDE = []\n",
    "plotting = True\n",
    "no_of_plotted_trajectories = 20\n",
    "\n",
    "for idx, agent_id in enumerate(test_agent_ids):\n",
    "\n",
    "    seen_x = np.array(data[data['agent_id'] == agent_id].iloc[:5]['pos_x'])\n",
    "    seen_y = np.array(data[data['agent_id'] == agent_id].iloc[:5]['pos_y'])\n",
    "    \n",
    "    seen_vel_x = np.array(data[data['agent_id'] == agent_id].iloc[1:5]['vel_x'])\n",
    "    seen_vel_y = np.array(data[data['agent_id'] == agent_id].iloc[1:5]['vel_y'])\n",
    "    \n",
    "    unseen_x = np.array(data[data['agent_id'] == agent_id].iloc[5:10]['pos_x'])\n",
    "    unseen_y = np.array(data[data['agent_id'] == agent_id].iloc[5:10]['pos_y'])\n",
    "\n",
    "    pred_vel_x, pred_vel_y = model.predict(np.array([np.column_stack((seen_vel_x, seen_vel_y))]))\n",
    "    pred_vel_x = pred_vel_x[0] # destructure the prediction array as it is [[x1, x2, x3...]]\n",
    "    pred_vel_y = pred_vel_y[0]\n",
    "    \n",
    "    pred_coord_x = [seen_x[-1] + pred_vel_x[0]]\n",
    "    pred_coord_y = [seen_y[-1] + pred_vel_y[0]]\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        pred_coord_x.append(pred_coord_x[-1] + pred_vel_x[i])\n",
    "        pred_coord_y.append(pred_coord_y[-1] + pred_vel_y[i])\n",
    "\n",
    "    FDE = np.sqrt((pred_coord_x[-1] - unseen_x[-1])**2 + (pred_coord_y[-1] - unseen_y[-1])**2)\n",
    "    all_FDE.append(FDE)\n",
    "    # plot data\n",
    "    if plotting and idx < no_of_plotted_trajectories:\n",
    "        plt.axes().set_aspect('equal')\n",
    "\n",
    "        plt.plot(np.append(seen_x, unseen_x), np.append(seen_y, unseen_y), color='g')\n",
    "        plt.scatter(np.append(seen_x, unseen_x), np.append(seen_y, unseen_y), color='g')\n",
    "\n",
    "        plt.plot(pred_coord_x, pred_coord_y, color='r')\n",
    "        plt.scatter(pred_coord_x, pred_coord_y, color='r')\n",
    "\n",
    "        plt.scatter(seen_x[0], seen_y[0], color='b')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "print(\"Average FDE: \", np.mean(all_FDE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cfedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_agent_ids))\n",
    "print(len(train_agent_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41be28",
   "metadata": {},
   "source": [
    "## Testing different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e-hnN5tT4NM0",
   "metadata": {
    "id": "e-hnN5tT4NM0"
   },
   "outputs": [],
   "source": [
    "# NN architecture\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "inp = Input((5,2))\n",
    "\n",
    "x = Flatten()(inp)\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "\n",
    "out1 = Dense(5, activation='linear')(x)    \n",
    "out2 = Dense(5, activation='linear')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db44051",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "batch_sizes = [1, 4, 8, 16, 32]\n",
    "loss_functions = ['mean_squared_error', root_mean_squared_error]\n",
    "\n",
    "all_train_RMSE = []\n",
    "all_test_RMSE = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for loss_function in loss_functions:\n",
    "            model = Model(inputs=inp, outputs=[out1,out2])\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss = loss_function)\n",
    "\n",
    "            model.fit(train[0], [train[1], train[2]], epochs=200, batch_size=32, verbose=0)\n",
    "            \n",
    "            trainScore = model.evaluate(train[0], [train[1], train[2]], verbose=0)\n",
    "            testScore = model.evaluate(test[0], [test[1], test[2]], verbose=0)\n",
    "            \n",
    "            if loss_function == 'mean_squared_error':\n",
    "                all_train_RMSE.append(np.sqrt(trainScore[0]))\n",
    "                all_test_RMSE.append(np.sqrt(testScore[0]))\n",
    "                #Wprint(\"Learning rate: %.3f, batch size: %i, loss function: %s\" % (learning_rate, batch_size, loss_function))\n",
    "                #print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], np.sqrt(trainScore[0])))\n",
    "                #print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], np.sqrt(testScore[0])))\n",
    "            else:\n",
    "                all_train_RMSE.append(trainScore[0])\n",
    "                all_test_RMSE.append(testScore[0])\n",
    "                #print(\"Learning rate: %.3f, batch size: %i, loss function: %s\" % (learning_rate, batch_size, loss_function.__name__))\n",
    "                #print('Train Score: %.2f RMSE' % (trainScore[0]))\n",
    "                #print('Test Score: %.2f RMSE' % (testScore[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb279c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate = None\n",
    "best_batch_size = None\n",
    "best_loss_function = None\n",
    "best_test_rmse = min(all_test_RMSE)\n",
    "best_test_rmse_idx = all_test_RMSE.index(best_test_rmse)\n",
    "\n",
    "rmse_idx = 0\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for loss_function in loss_functions:\n",
    "            \n",
    "            if rmse_idx == best_test_rmse_idx:\n",
    "                best_learning_rate = learning_rate\n",
    "                best_batch_size = batch_size\n",
    "                if loss_function == 'mean_squared_error':\n",
    "                    best_loss_function = loss_function\n",
    "                else:\n",
    "                    best_loss_function = loss_function.__name__\n",
    "            \n",
    "            if loss_function == 'mean_squared_error':\n",
    "                print(\"Learning rate: %.3f, batch size: %i, loss function: %s\" % (learning_rate, batch_size, loss_function))\n",
    "            else:\n",
    "                print(\"Learning rate: %.3f, batch size: %i, loss function: %s\" % (learning_rate, batch_size, loss_function.__name__))\n",
    "            print('Train Score: %.2f RMSE' % (all_train_RMSE[rmse_idx]))\n",
    "            print('Test Score: %.2f RMSE' % (all_test_RMSE[rmse_idx]))\n",
    "            print()\n",
    "            rmse_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best test RMSE: \", best_test_rmse)\n",
    "print(\"Best learning rate: \", best_learning_rate)\n",
    "print(\"Best batch size: \", best_batch_size)\n",
    "print(\"Best loss function: \", best_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67cb5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Edinburgh-NN-over-all-data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
