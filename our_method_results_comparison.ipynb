{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1740c3",
   "metadata": {},
   "source": [
    "# Results comparison between Trajectron++ and our method\n",
    "Comparing the results of our method with the Trajectron++ paper's method. The goal of the comparison is to evaluate Trajectron++ on a different dataset than it was trained on to allow more fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cfa130bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import generative_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd1a8c",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1ac04490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_FDE(pred_x, pred_y, test_x, test_y):\n",
    "\n",
    "    final_displacement_x = pred_x[-1] - test_x[-1]\n",
    "    final_displacement_y = pred_y[-1] - test_y[-1]\n",
    "    FDE = np.sqrt(final_displacement_x**2 + final_displacement_y**2)\n",
    "    \n",
    "    return FDE\n",
    "\n",
    "def calculate_ADE(pred_x, pred_y, test_x, test_y):\n",
    "    assert len(pred_x) == len(test_x)\n",
    "    total_displacement_error = 0\n",
    "    for point_idx in range(len(test_x)):\n",
    "        displacement_error = np.sqrt((pred_x[point_idx] - test_x[point_idx])**2 + (pred_y[point_idx] - test_y[point_idx])**2)\n",
    "        total_displacement_error += displacement_error\n",
    "\n",
    "    return total_displacement_error/len(pred_x)\n",
    "\n",
    "## The evaluation logic for Trajectron++ loops over the frames and predicts the future trajectories \n",
    "## for each node present in the current frame\n",
    "## Each node has to have at least 7 historical points and 12 future points\n",
    "def get_total_predictable_slices(data):\n",
    "    total_predictable_steps = 0\n",
    "    for i in pd.unique(data.node_id):\n",
    "        #print(len(test[test.node_id == i]))\n",
    "        total_predictable_steps += len(data[data.node_id == i]) - 19\n",
    "    return total_predictable_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "907d43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_data):\n",
    "    data = input_data.copy()\n",
    "    data['frame_id'] = pd.to_numeric(data['frame_id'], downcast='integer')\n",
    "    data['track_id'] = pd.to_numeric(data['track_id'], downcast='integer')\n",
    "\n",
    "    data['frame_id'] = data['frame_id'] // 10\n",
    "\n",
    "    data['frame_id'] -= data['frame_id'].min()\n",
    "\n",
    "    data['node_type'] = 'PEDESTRIAN'\n",
    "    data['node_id'] = data['track_id'].astype(str)\n",
    "    data.sort_values('frame_id', inplace=True)\n",
    "\n",
    "    data['pos_x'] = data['pos_x'] - data['pos_x'].mean()\n",
    "    data['pos_y'] = data['pos_y'] - data['pos_y'].mean()\n",
    "    \n",
    "    # Select only such nodes which have enough data to predict on (8 historical timesteps, 12 future)\n",
    "    v = data.node_id.value_counts()\n",
    "    data = data[data.node_id.isin(v.index[v.gt(19)])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce140413",
   "metadata": {},
   "source": [
    "## Method evaluation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "46e27d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_our_method(data, params, dataset_title='', clustering_method='KMeans', smoothing=False):\n",
    "    our_fde_best_of = []\n",
    "    our_ade_best_of = []\n",
    "    our_fde_single = []\n",
    "    our_ade_single = []\n",
    "\n",
    "    # Loop over the dataset frame by frame\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='Ours - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        \n",
    "        # Loop over all agents in the current frame\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check for 8 points of history for current agent\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Check for 12 points of future for current agent\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    all_pred_x, all_pred_y, weights = generative_model.predict(x_data, y_data, params, trajectory_length=12, clustering_method=clustering_method, smoothing=smoothing)\n",
    "                    assert len(all_pred_x[0]) == 12\n",
    "                    \n",
    "                    # This section is for producing a single trajectory (averaging all representative preds)\n",
    "                    avg_x = np.average(all_pred_x, axis=0, weights=weights)\n",
    "                    avg_y = np.average(all_pred_y, axis=0, weights=weights)\n",
    "                    #avg_x = np.mean(all_pred_x, axis=0)\n",
    "                    #avg_y = np.mean(all_pred_y, axis=0)\n",
    "                    avg_fde = calculate_FDE(avg_x, avg_y, x_gt, y_gt)\n",
    "                    avg_ade = calculate_ADE(avg_x, avg_y, x_gt, y_gt)\n",
    "                    our_fde_single.append(avg_fde)\n",
    "                    our_ade_single.append(avg_ade)\n",
    "                        \n",
    "                        \n",
    "                    # This section is for finding the best trajectories out of many in terms of FDE and ADE\n",
    "                    best_fde = None\n",
    "                    best_ade = None\n",
    "\n",
    "                    for i in range(len(all_pred_x)):\n",
    "                        current_pred_x = all_pred_x[i]\n",
    "                        current_pred_y = all_pred_y[i]\n",
    "\n",
    "                        fde = calculate_FDE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_fde == None or fde < best_fde:\n",
    "                            best_fde = fde\n",
    "\n",
    "                        ade = calculate_ADE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_ade == None or ade < best_ade:\n",
    "                            best_ade = ade\n",
    "\n",
    "                    our_fde_best_of.append(best_fde)\n",
    "                    our_ade_best_of.append(best_ade)\n",
    "                    \n",
    "    return our_fde_best_of, our_ade_best_of, our_fde_single, our_ade_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "781a48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cvm_with_scenarios(data, dataset_title='', history_length=8):\n",
    "    tot = 0\n",
    "    our_fde_best_of = []\n",
    "    our_ade_best_of = []\n",
    "\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='CVM - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        #print(frame_data)\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check if at least 7 historical points are present\n",
    "            # PS: It might be so that the prediction starts at the 8th step instead of 7th? Edited the code to do this at the moment\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Not sure why there has to be more than 12 frames to the future (at least 13) but it's the\n",
    "                # only way to get the number of trajectron++ eval predictions to match up\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    tot += 1\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    all_pred_x, all_pred_y = [], []\n",
    "                    \n",
    "                    # CVM\n",
    "                    for i in range(20):\n",
    "                        history_x = x_data[-history_length:]\n",
    "                        history_y = y_data[-history_length:]\n",
    "                        assert len(history_x) == history_length\n",
    "                        \n",
    "                        if i == 0:\n",
    "                            vel_x = [history_x[i] - history_x[i-1] for i in range(1, len(history_x))]\n",
    "                            vel_y = [history_y[i] - history_y[i-1] for i in range(1, len(history_y))]\n",
    "                        else:\n",
    "                            vel_x = [(history_x[i] - history_x[i-1]) + np.random.normal(0, 1) for i in range(1, len(history_x))]\n",
    "                            vel_y = [(history_y[i] - history_y[i-1]) + np.random.normal(0, 1) for i in range(1, len(history_y))]\n",
    "                        \n",
    "                        assert len(vel_x) == history_length-1\n",
    "                        avg_vel_x = np.mean(vel_x)\n",
    "                        avg_vel_y = np.mean(vel_y)\n",
    "                        \n",
    "                        pred_x = [x_data[-1] + i*avg_vel_x for i in range(1, 13)]\n",
    "                        pred_y = [y_data[-1] + i*avg_vel_y for i in range(1, 13)]\n",
    "                        assert len(pred_x) == 12\n",
    "                        \n",
    "                        all_pred_x.append(pred_x)\n",
    "                        all_pred_y.append(pred_y)\n",
    "                    \n",
    "\n",
    "                    best_fde = None\n",
    "                    best_ade = None\n",
    "                    for i in range(len(all_pred_x)):\n",
    "                        current_pred_x = all_pred_x[i]\n",
    "                        current_pred_y = all_pred_y[i]\n",
    "\n",
    "                        fde = calculate_FDE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_fde == None or fde < best_fde:\n",
    "                            best_fde = fde\n",
    "\n",
    "                        ade = calculate_ADE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_ade == None or ade < best_ade:\n",
    "                            best_ade = ade\n",
    "\n",
    "                    our_fde_best_of.append(best_fde)\n",
    "                    our_ade_best_of.append(best_ade)\n",
    "                    \n",
    "    return our_fde_best_of, our_ade_best_of\n",
    "\n",
    "def evaluate_cvm(data, dataset_title='', history_length=8):\n",
    "    tot = 0\n",
    "    all_fde = []\n",
    "    all_ade = []\n",
    "\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='CVM - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        #print(frame_data)\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check if at least 7 historical points are present\n",
    "            # PS: It might be so that the prediction starts at the 8th step instead of 7th? Edited the code to do this at the moment\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Not sure why there has to be more than 12 frames to the future (at least 13) but it's the\n",
    "                # only way to get the number of trajectron++ eval predictions to match up\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    tot += 1\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    history_x = x_data[-history_length:]\n",
    "                    history_y = y_data[-history_length:]\n",
    "                    assert len(history_x) == history_length\n",
    "\n",
    "                    vel_x = [history_x[i] - history_x[i-1] for i in range(1, len(history_x))]\n",
    "                    vel_y = [history_y[i] - history_y[i-1] for i in range(1, len(history_y))]\n",
    "\n",
    "                    assert len(vel_x) == history_length-1\n",
    "                    avg_vel_x = np.mean(vel_x)\n",
    "                    avg_vel_y = np.mean(vel_y)\n",
    "\n",
    "                    pred_x = [x_data[-1] + i*avg_vel_x for i in range(1, 13)]\n",
    "                    pred_y = [y_data[-1] + i*avg_vel_y for i in range(1, 13)]\n",
    "                    assert len(pred_x) == 12\n",
    "\n",
    "                    fde = calculate_FDE(pred_x, pred_y, x_gt, y_gt)\n",
    "                    ade = calculate_ADE(pred_x, pred_y, x_gt, y_gt)\n",
    "\n",
    "                    all_fde.append(fde)\n",
    "                    all_ade.append(ade)\n",
    "                    \n",
    "    return all_fde, all_ade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a687f69",
   "metadata": {},
   "source": [
    "## Automated results comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ef2ab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trajectron_data(trajectron_resultset_name, base_folder='./trajectron++/results_paper_version/', suffix='best_of'):\n",
    "    trajectron_fde = []\n",
    "    with open(base_folder + trajectron_resultset_name + '_fde_' + suffix + '.csv', mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            trajectron_fde.append(float(row['value']))\n",
    "\n",
    "    trajectron_ade = []\n",
    "    with open(base_folder + trajectron_resultset_name + '_ade_'+ suffix + '.csv', mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            trajectron_ade.append(float(row['value']))\n",
    "            \n",
    "    return trajectron_fde, trajectron_ade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3cf14c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_datasets(our_method_params, datasets, trajectron_resultset_names, trajectron_ar3_resultset_names, evaluate_most_likely=False, evaluate_with_interactions=False):\n",
    "    base_path = './raw_data/'\n",
    "\n",
    "    ours_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    trajectron_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    trajectron_ar3_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    cvm_long_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    cvm_short_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "\n",
    "    for dataset_idx, dataset in enumerate(datasets):\n",
    "        our_fde_bo20, our_ade_bo20 = [], []\n",
    "        our_fde_most_likely, our_ade_most_likely = [], []\n",
    "        \n",
    "        cvm_fde_bo20, cvm_ade_bo20 = [], []\n",
    "        cvm_short_fde_bo20, cvm_short_ade_bo20 = [], []\n",
    "        \n",
    "        cvm_fde_ml, cvm_ade_ml = [], []\n",
    "        cvm_short_fde_ml, cvm_short_ade_ml = [], []\n",
    "        \n",
    "        for scene_idx, scene in enumerate(dataset):\n",
    "            data = pd.read_csv(base_path + scene, sep='\\t', index_col=False, header=None)\n",
    "            data.columns = ['frame_id', 'track_id', 'pos_x', 'pos_y']\n",
    "\n",
    "            data = process_data(data)\n",
    "\n",
    "            ## Ours\n",
    "            our_fde_best_of_20, our_ade_best_of_20, our_fde_single, our_ade_single = evaluate_our_method(data, our_method_params[dataset_idx], dataset_title=trajectron_resultset_names[dataset_idx], smoothing=True)\n",
    "            our_fde_bo20 += our_fde_best_of_20\n",
    "            our_ade_bo20 += our_ade_best_of_20\n",
    "            if evaluate_most_likely:\n",
    "                our_fde_most_likely += our_fde_single\n",
    "                our_ade_most_likely += our_ade_single\n",
    "\n",
    "            ## CVM\n",
    "            cvm_fde_best_of, cvm_ade_best_of = evaluate_cvm_with_scenarios(data, dataset_title=trajectron_resultset_names[dataset_idx])\n",
    "            cvm_fde_bo20 += cvm_fde_best_of\n",
    "            cvm_ade_bo20 += cvm_ade_best_of\n",
    "            \n",
    "            cvm_fde_short_history_best_of, cvm_ade_short_history_best_of = evaluate_cvm_with_scenarios(data, dataset_title=trajectron_resultset_names[dataset_idx], history_length=2)\n",
    "            cvm_short_fde_bo20 += cvm_fde_short_history_best_of\n",
    "            cvm_short_ade_bo20 += cvm_ade_short_history_best_of\n",
    "\n",
    "            if evaluate_most_likely:\n",
    "                cvm_fde, cvm_ade = evaluate_cvm(data, dataset_title=trajectron_resultset_names[dataset_idx])\n",
    "                cvm_fde_ml += cvm_fde\n",
    "                cvm_ade_ml += cvm_ade\n",
    "                \n",
    "                cvm_fde_short_history, cvm_ade_short_history = evaluate_cvm(data, dataset_title=trajectron_resultset_names[dataset_idx], history_length=2)\n",
    "                cvm_short_fde_ml += cvm_fde_short_history\n",
    "                cvm_short_ade_ml += cvm_ade_short_history\n",
    "\n",
    "        # add our and CVM results to the data dict\n",
    "        ours_results['BEST_OF_20']['FDE'].append(np.mean(our_fde_bo20))\n",
    "        ours_results['BEST_OF_20']['ADE'].append(np.mean(our_ade_bo20))\n",
    "        if evaluate_most_likely:\n",
    "            ours_results['MOST_LIKELY']['FDE'].append(np.mean(our_fde_most_likely))\n",
    "            ours_results['MOST_LIKELY']['ADE'].append(np.mean(our_ade_most_likely))\n",
    "            \n",
    "        cvm_long_results['BEST_OF_20']['FDE'].append(np.mean(cvm_fde_best_of))\n",
    "        cvm_long_results['BEST_OF_20']['ADE'].append(np.mean(cvm_ade_best_of))\n",
    "        cvm_short_results['BEST_OF_20']['FDE'].append(np.mean(cvm_fde_short_history_best_of))\n",
    "        cvm_short_results['BEST_OF_20']['ADE'].append(np.mean(cvm_ade_short_history_best_of))\n",
    "        if evaluate_most_likely:\n",
    "            cvm_long_results['MOST_LIKELY']['FDE'].append(np.mean(cvm_fde))\n",
    "            cvm_long_results['MOST_LIKELY']['ADE'].append(np.mean(cvm_ade))\n",
    "            cvm_short_results['MOST_LIKELY']['FDE'].append(np.mean(cvm_fde_short_history))\n",
    "            cvm_short_results['MOST_LIKELY']['ADE'].append(np.mean(cvm_ade_short_history))\n",
    "                \n",
    "        ## Trajectron\n",
    "        trajectron_fde, trajectron_ade = read_trajectron_data(trajectron_resultset_names[dataset_idx])\n",
    "        trajectron_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_fde))\n",
    "        trajectron_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ade))\n",
    "        \n",
    "        if evaluate_most_likely:\n",
    "            trajectron_fde, trajectron_ade = read_trajectron_data(trajectron_resultset_names[dataset_idx], suffix='most_likely')\n",
    "            trajectron_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_fde))\n",
    "            trajectron_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ade))\n",
    "        \n",
    "        ## Trajectron with interactions\n",
    "        if evaluate_with_interactions:\n",
    "            trajectron_ar3_fde, trajectron_ar3_ade = read_trajectron_data(trajectron_ar3_resultset_names[dataset_idx])\n",
    "            trajectron_ar3_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_ar3_fde))\n",
    "            trajectron_ar3_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ar3_ade))\n",
    "\n",
    "            if evaluate_most_likely:\n",
    "                trajectron_ar3_fde, trajectron_ar3_ade = read_trajectron_data(trajectron_resultset_names[dataset_idx], suffix='most_likely')\n",
    "                trajectron_ar3_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_ar3_fde))\n",
    "                trajectron_ar3_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ar3_ade))\n",
    "\n",
    "        # make sure that there is no discrepancy between our data processing and trajectron evaluation results size\n",
    "        if len(dataset) == 1:\n",
    "            num_predictable_trajectories = get_total_predictable_slices(data)\n",
    "            assert len(trajectron_fde) == num_predictable_trajectories\n",
    "            assert len(trajectron_ade) == num_predictable_trajectories\n",
    "    \n",
    "    return [\n",
    "        ours_results,\n",
    "        trajectron_results,\n",
    "        trajectron_ar3_results,\n",
    "        cvm_long_results,\n",
    "        cvm_short_results\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac9d82",
   "metadata": {},
   "source": [
    "### Running the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a429d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ours - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:33<00:00, 14.31it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:01<00:00, 470.62it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 539.29it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 693.45it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 699.49it/s]\n",
      "Ours - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [01:52<00:00,  8.11it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [00:03<00:00, 245.76it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [00:03<00:00, 272.82it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [00:02<00:00, 341.07it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [00:02<00:00, 341.37it/s]\n",
      "Ours - univ_vel: 100%|████████████████████████████████████████████████████████████████████████| 444/444 [21:42<00:00,  2.93s/it]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 444/444 [01:14<00:00,  5.98it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 444/444 [01:10<00:00,  6.31it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 444/444 [01:01<00:00,  7.16it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 444/444 [01:02<00:00,  7.15it/s]\n",
      "Ours - univ_vel: 100%|████████████████████████████████████████████████████████████████████████| 541/541 [15:22<00:00,  1.70s/it]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 541/541 [00:48<00:00, 11.08it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 541/541 [00:45<00:00, 11.79it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 541/541 [00:40<00:00, 13.49it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 541/541 [00:40<00:00, 13.50it/s]\n",
      "Ours - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [03:40<00:00,  3.92it/s]\n",
      "CVM - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [00:07<00:00, 123.33it/s]\n",
      "CVM - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [00:06<00:00, 136.51it/s]\n",
      "CVM - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [00:05<00:00, 172.29it/s]\n",
      "CVM - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [00:05<00:00, 172.17it/s]\n",
      "Ours - zara2_vel_modified: 100%|████████████████████████████████████████████████████████████| 1052/1052 [09:03<00:00,  1.94it/s]\n",
      "CVM - zara2_vel_modified: 100%|█████████████████████████████████████████████████████████████| 1052/1052 [00:20<00:00, 51.37it/s]\n",
      "CVM - zara2_vel_modified: 100%|█████████████████████████████████████████████████████████████| 1052/1052 [00:18<00:00, 56.35it/s]\n",
      "CVM - zara2_vel_modified: 100%|█████████████████████████████████████████████████████████████| 1052/1052 [00:15<00:00, 68.43it/s]\n",
      "CVM - zara2_vel_modified: 100%|█████████████████████████████████████████████████████████████| 1052/1052 [00:15<00:00, 68.51it/s]\n"
     ]
    }
   ],
   "source": [
    "eth_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 500, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.025, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.15, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.5, 0.75, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 9, 6, 4]\n",
    "}\n",
    "\n",
    "hotel_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 500, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.0, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.1, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 11, 8]\n",
    "}\n",
    "\n",
    "zara1_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 500, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.0, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.2, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 14, 5]\n",
    "}\n",
    "\n",
    "zara2_params = {\n",
    "    'NOISE': 0.1, \n",
    "    'NO_OF_TRAJECTORIES': 500, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.025, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.2, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.1, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 14, 5]\n",
    "}\n",
    "\n",
    "univ_params = {\n",
    "    'NOISE': 0.025, \n",
    "    'NO_OF_TRAJECTORIES': 500, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.05, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.1, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.5, 0.75, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 9, 6, 4]\n",
    "}\n",
    "\n",
    "\n",
    "our_method_params = [eth_params, hotel_params, univ_params, zara1_params, zara2_params]\n",
    "\n",
    "datasets = [\n",
    "    ['eth/test/biwi_eth.txt'], \n",
    "    ['hotel/test/biwi_hotel.txt'], \n",
    "    ['univ/test/students001.txt', 'univ/test/students003.txt'],\n",
    "    ['zara1/test/crowds_zara01.txt'], \n",
    "    ['zara2/test/crowds_zara02.txt'],\n",
    "]\n",
    "\n",
    "trajectron_resultset_names = [\n",
    "    'eth_vel', \n",
    "    'hotel_vel',\n",
    "    'univ_vel',\n",
    "    'zara1_vel_modified', \n",
    "    'zara2_vel_modified',\n",
    "]\n",
    "\n",
    "trajectron_ar3_resultset_names = [\n",
    "    'eth_ar3', \n",
    "    'hotel_ar3', \n",
    "    'univ_ar3',\n",
    "    'zara1_ar3', \n",
    "    'zara2_ar3',\n",
    "]\n",
    "\n",
    "res = evaluate_all_datasets(our_method_params, datasets, trajectron_resultset_names, trajectron_ar3_resultset_names, evaluate_most_likely=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8bedb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ours_results = res[0]\n",
    "trajectron_results = res[1]\n",
    "trajectron_ar3_results = res[2]\n",
    "cvm_long_results = res[3]\n",
    "cvm_short_results = res[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7260ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgan_results = {\n",
    "    'BEST_OF_20': {\n",
    "        'FDE': [1.52, 1.61, 0.69, 1.26, 0.84], \n",
    "        'ADE': [0.81, 0.72, 0.60, 0.34, 0.42]\n",
    "    }, \n",
    "    'MOST_LIKELY': {\n",
    "        'FDE': [2.21, 2.18, 1.28, 0.91, 1.11], \n",
    "        'ADE': [ 1.13, 1.01, 0.60, 0.42, 0.52]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ab25de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [\n",
    "    'ETH', \n",
    "    'Hotel', \n",
    "    'Univ',\n",
    "    'Zara 1', \n",
    "    'Zara 2',\n",
    "    \n",
    "]\n",
    "\n",
    "df_data_best_of_20_fde = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Social-GAN': pd.Series(sgan_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['BEST_OF_20']['FDE'], index = index),\n",
    "    #'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['BEST_OF_20']['FDE'], index = index)\n",
    "}\n",
    "\n",
    "df_best_of_20_fde = pd.DataFrame(df_data_best_of_20_fde)\n",
    "\n",
    "df_data_best_of_20_ade = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Social-GAN': pd.Series(sgan_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['BEST_OF_20']['ADE'], index = index),\n",
    "    #'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['BEST_OF_20']['ADE'], index = index)\n",
    "}\n",
    "\n",
    "df_best_of_20_ade = pd.DataFrame(df_data_best_of_20_ade)\n",
    "\n",
    "df_data_most_likely_fde = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Social-GAN': pd.Series(sgan_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    #'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['MOST_LIKELY']['FDE'], index = index)\n",
    "}\n",
    "\n",
    "df_most_likely_fde = pd.DataFrame(df_data_most_likely_fde)\n",
    "\n",
    "df_data_most_likely_ade = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Social-GAN': pd.Series(sgan_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    #'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['MOST_LIKELY']['ADE'], index = index)\n",
    "}\n",
    "\n",
    "df_most_likely_ade = pd.DataFrame(df_data_most_likely_ade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64a9ed",
   "metadata": {},
   "source": [
    "### Best of 20 - FDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "bf1c7482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d70e4_row0_col4, #T_d70e4_row1_col4, #T_d70e4_row2_col4, #T_d70e4_row3_col3, #T_d70e4_row4_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d70e4_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CVM (8pt history)</th>\n",
       "      <th class=\"col_heading level0 col1\" >CVM (2pt history)</th>\n",
       "      <th class=\"col_heading level0 col2\" >Social-GAN</th>\n",
       "      <th class=\"col_heading level0 col3\" >Trajectron++</th>\n",
       "      <th class=\"col_heading level0 col4\" >Ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d70e4_level0_row0\" class=\"row_heading level0 row0\" >ETH</th>\n",
       "      <td id=\"T_d70e4_row0_col0\" class=\"data row0 col0\" >1.108</td>\n",
       "      <td id=\"T_d70e4_row0_col1\" class=\"data row0 col1\" >1.758</td>\n",
       "      <td id=\"T_d70e4_row0_col2\" class=\"data row0 col2\" >1.520</td>\n",
       "      <td id=\"T_d70e4_row0_col3\" class=\"data row0 col3\" >0.812</td>\n",
       "      <td id=\"T_d70e4_row0_col4\" class=\"data row0 col4\" >0.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d70e4_level0_row1\" class=\"row_heading level0 row1\" >Hotel</th>\n",
       "      <td id=\"T_d70e4_row1_col0\" class=\"data row1 col0\" >0.363</td>\n",
       "      <td id=\"T_d70e4_row1_col1\" class=\"data row1 col1\" >0.590</td>\n",
       "      <td id=\"T_d70e4_row1_col2\" class=\"data row1 col2\" >1.610</td>\n",
       "      <td id=\"T_d70e4_row1_col3\" class=\"data row1 col3\" >0.197</td>\n",
       "      <td id=\"T_d70e4_row1_col4\" class=\"data row1 col4\" >0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d70e4_level0_row2\" class=\"row_heading level0 row2\" >Univ</th>\n",
       "      <td id=\"T_d70e4_row2_col0\" class=\"data row2 col0\" >0.971</td>\n",
       "      <td id=\"T_d70e4_row2_col1\" class=\"data row2 col1\" >1.220</td>\n",
       "      <td id=\"T_d70e4_row2_col2\" class=\"data row2 col2\" >0.690</td>\n",
       "      <td id=\"T_d70e4_row2_col3\" class=\"data row2 col3\" >0.450</td>\n",
       "      <td id=\"T_d70e4_row2_col4\" class=\"data row2 col4\" >0.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d70e4_level0_row3\" class=\"row_heading level0 row3\" >Zara 1</th>\n",
       "      <td id=\"T_d70e4_row3_col0\" class=\"data row3 col0\" >0.793</td>\n",
       "      <td id=\"T_d70e4_row3_col1\" class=\"data row3 col1\" >0.884</td>\n",
       "      <td id=\"T_d70e4_row3_col2\" class=\"data row3 col2\" >1.260</td>\n",
       "      <td id=\"T_d70e4_row3_col3\" class=\"data row3 col3\" >0.342</td>\n",
       "      <td id=\"T_d70e4_row3_col4\" class=\"data row3 col4\" >0.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d70e4_level0_row4\" class=\"row_heading level0 row4\" >Zara 2</th>\n",
       "      <td id=\"T_d70e4_row4_col0\" class=\"data row4 col0\" >0.555</td>\n",
       "      <td id=\"T_d70e4_row4_col1\" class=\"data row4 col1\" >0.656</td>\n",
       "      <td id=\"T_d70e4_row4_col2\" class=\"data row4 col2\" >0.840</td>\n",
       "      <td id=\"T_d70e4_row4_col3\" class=\"data row4 col3\" >0.253</td>\n",
       "      <td id=\"T_d70e4_row4_col4\" class=\"data row4 col4\" >0.309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13763c3d0>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-means without smoothing\n",
    "df_best_of_20_fde.style.highlight_min(color = 'lightgreen', axis = 1).format(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "760f5665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} & {CVM (8pt history)} & {CVM (2pt history)} & {Social-GAN} & {Trajectron++} & {Ours} \\\\\n",
      "\\midrule\n",
      "ETH & 1.108 & 1.758 & 1.520 & 0.812 & \\textbf{0.642} \\\\\n",
      "Hotel & 0.363 & 0.590 & 1.610 & 0.197 & \\textbf{0.180} \\\\\n",
      "Univ & 0.971 & 1.220 & 0.690 & 0.450 & \\textbf{0.448} \\\\\n",
      "Zara 1 & 0.793 & 0.884 & 1.260 & \\textbf{0.342} & 0.416 \\\\\n",
      "Zara 2 & 0.555 & 0.656 & 0.840 & \\textbf{0.253} & 0.309 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(df_best_of_20_fde.to_latex(header=False))\n",
    "\n",
    "print(df_best_of_20_fde.style.highlight_min(props='textbf:--rwrap', axis=1).format(precision=3).to_latex(hrules=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296114d",
   "metadata": {},
   "source": [
    "### Best of 20 - ADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "286ec7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6b953_row0_col3, #T_6b953_row1_col3, #T_6b953_row2_col3, #T_6b953_row3_col3, #T_6b953_row4_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6b953_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CVM (8pt history)</th>\n",
       "      <th class=\"col_heading level0 col1\" >CVM (2pt history)</th>\n",
       "      <th class=\"col_heading level0 col2\" >Social-GAN</th>\n",
       "      <th class=\"col_heading level0 col3\" >Trajectron++</th>\n",
       "      <th class=\"col_heading level0 col4\" >Ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6b953_level0_row0\" class=\"row_heading level0 row0\" >ETH</th>\n",
       "      <td id=\"T_6b953_row0_col0\" class=\"data row0 col0\" >0.641</td>\n",
       "      <td id=\"T_6b953_row0_col1\" class=\"data row0 col1\" >0.899</td>\n",
       "      <td id=\"T_6b953_row0_col2\" class=\"data row0 col2\" >0.810</td>\n",
       "      <td id=\"T_6b953_row0_col3\" class=\"data row0 col3\" >0.396</td>\n",
       "      <td id=\"T_6b953_row0_col4\" class=\"data row0 col4\" >0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b953_level0_row1\" class=\"row_heading level0 row1\" >Hotel</th>\n",
       "      <td id=\"T_6b953_row1_col0\" class=\"data row1 col0\" >0.205</td>\n",
       "      <td id=\"T_6b953_row1_col1\" class=\"data row1 col1\" >0.308</td>\n",
       "      <td id=\"T_6b953_row1_col2\" class=\"data row1 col2\" >0.720</td>\n",
       "      <td id=\"T_6b953_row1_col3\" class=\"data row1 col3\" >0.115</td>\n",
       "      <td id=\"T_6b953_row1_col4\" class=\"data row1 col4\" >0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b953_level0_row2\" class=\"row_heading level0 row2\" >Univ</th>\n",
       "      <td id=\"T_6b953_row2_col0\" class=\"data row2 col0\" >0.527</td>\n",
       "      <td id=\"T_6b953_row2_col1\" class=\"data row2 col1\" >0.572</td>\n",
       "      <td id=\"T_6b953_row2_col2\" class=\"data row2 col2\" >0.600</td>\n",
       "      <td id=\"T_6b953_row2_col3\" class=\"data row2 col3\" >0.205</td>\n",
       "      <td id=\"T_6b953_row2_col4\" class=\"data row2 col4\" >0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b953_level0_row3\" class=\"row_heading level0 row3\" >Zara 1</th>\n",
       "      <td id=\"T_6b953_row3_col0\" class=\"data row3 col0\" >0.416</td>\n",
       "      <td id=\"T_6b953_row3_col1\" class=\"data row3 col1\" >0.409</td>\n",
       "      <td id=\"T_6b953_row3_col2\" class=\"data row3 col2\" >0.340</td>\n",
       "      <td id=\"T_6b953_row3_col3\" class=\"data row3 col3\" >0.155</td>\n",
       "      <td id=\"T_6b953_row3_col4\" class=\"data row3 col4\" >0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b953_level0_row4\" class=\"row_heading level0 row4\" >Zara 2</th>\n",
       "      <td id=\"T_6b953_row4_col0\" class=\"data row4 col0\" >0.294</td>\n",
       "      <td id=\"T_6b953_row4_col1\" class=\"data row4 col1\" >0.304</td>\n",
       "      <td id=\"T_6b953_row4_col2\" class=\"data row4 col2\" >0.420</td>\n",
       "      <td id=\"T_6b953_row4_col3\" class=\"data row4 col3\" >0.115</td>\n",
       "      <td id=\"T_6b953_row4_col4\" class=\"data row4 col4\" >0.182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1377edd60>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-means without smoothing\n",
    "df_best_of_20_ade.style.highlight_min(color = 'lightgreen', axis = 1).format(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "837c81e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "{} & {CVM (8pt history)} & {CVM (2pt history)} & {Social-GAN} & {Trajectron++} & {Ours} \\\\\n",
      "ETH & 0.641 & 0.899 & 0.810 & \\textbf{0.396} & 0.440 \\\\\n",
      "Hotel & 0.205 & 0.308 & 0.720 & \\textbf{0.115} & 0.125 \\\\\n",
      "Univ & 0.527 & 0.572 & 0.600 & \\textbf{0.205} & 0.264 \\\\\n",
      "Zara 1 & 0.416 & 0.409 & 0.340 & \\textbf{0.155} & 0.248 \\\\\n",
      "Zara 2 & 0.294 & 0.304 & 0.420 & \\textbf{0.115} & 0.182 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_best_of_20_ade.style.highlight_min(props='textbf:--rwrap', axis=1).format(precision=3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee6f79",
   "metadata": {},
   "source": [
    "### Single output - FDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0c3796e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5dd70_row0_col3, #T_5dd70_row1_col0, #T_5dd70_row2_col4, #T_5dd70_row3_col3, #T_5dd70_row4_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5dd70_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CVM (8pt history)</th>\n",
       "      <th class=\"col_heading level0 col1\" >CVM (2pt history)</th>\n",
       "      <th class=\"col_heading level0 col2\" >Social-GAN</th>\n",
       "      <th class=\"col_heading level0 col3\" >Trajectron++</th>\n",
       "      <th class=\"col_heading level0 col4\" >Ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5dd70_level0_row0\" class=\"row_heading level0 row0\" >ETH</th>\n",
       "      <td id=\"T_5dd70_row0_col0\" class=\"data row0 col0\" >2.303</td>\n",
       "      <td id=\"T_5dd70_row0_col1\" class=\"data row0 col1\" >2.282</td>\n",
       "      <td id=\"T_5dd70_row0_col2\" class=\"data row0 col2\" >2.210</td>\n",
       "      <td id=\"T_5dd70_row0_col3\" class=\"data row0 col3\" >1.615</td>\n",
       "      <td id=\"T_5dd70_row0_col4\" class=\"data row0 col4\" >1.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5dd70_level0_row1\" class=\"row_heading level0 row1\" >Hotel</th>\n",
       "      <td id=\"T_5dd70_row1_col0\" class=\"data row1 col0\" >0.462</td>\n",
       "      <td id=\"T_5dd70_row1_col1\" class=\"data row1 col1\" >0.614</td>\n",
       "      <td id=\"T_5dd70_row1_col2\" class=\"data row1 col2\" >2.180</td>\n",
       "      <td id=\"T_5dd70_row1_col3\" class=\"data row1 col3\" >0.499</td>\n",
       "      <td id=\"T_5dd70_row1_col4\" class=\"data row1 col4\" >0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5dd70_level0_row2\" class=\"row_heading level0 row2\" >Univ</th>\n",
       "      <td id=\"T_5dd70_row2_col0\" class=\"data row2 col0\" >1.576</td>\n",
       "      <td id=\"T_5dd70_row2_col1\" class=\"data row2 col1\" >1.369</td>\n",
       "      <td id=\"T_5dd70_row2_col2\" class=\"data row2 col2\" >1.280</td>\n",
       "      <td id=\"T_5dd70_row2_col3\" class=\"data row2 col3\" >1.205</td>\n",
       "      <td id=\"T_5dd70_row2_col4\" class=\"data row2 col4\" >1.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5dd70_level0_row3\" class=\"row_heading level0 row3\" >Zara 1</th>\n",
       "      <td id=\"T_5dd70_row3_col0\" class=\"data row3 col0\" >1.132</td>\n",
       "      <td id=\"T_5dd70_row3_col1\" class=\"data row3 col1\" >0.952</td>\n",
       "      <td id=\"T_5dd70_row3_col2\" class=\"data row3 col2\" >0.910</td>\n",
       "      <td id=\"T_5dd70_row3_col3\" class=\"data row3 col3\" >0.770</td>\n",
       "      <td id=\"T_5dd70_row3_col4\" class=\"data row3 col4\" >1.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5dd70_level0_row4\" class=\"row_heading level0 row4\" >Zara 2</th>\n",
       "      <td id=\"T_5dd70_row4_col0\" class=\"data row4 col0\" >0.860</td>\n",
       "      <td id=\"T_5dd70_row4_col1\" class=\"data row4 col1\" >0.724</td>\n",
       "      <td id=\"T_5dd70_row4_col2\" class=\"data row4 col2\" >1.110</td>\n",
       "      <td id=\"T_5dd70_row4_col3\" class=\"data row4 col3\" >0.589</td>\n",
       "      <td id=\"T_5dd70_row4_col4\" class=\"data row4 col4\" >0.804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1379260a0>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_most_likely_fde.style.highlight_min(color = 'lightgreen', axis = 1).format(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a89dbd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "{} & {CVM (8pt history)} & {CVM (2pt history)} & {Social-GAN} & {Trajectron++} & {Ours} \\\\\n",
      "ETH & 2.303 & 2.282 & 2.210 & \\textbf{1.615} & 1.917 \\\\\n",
      "Hotel & \\textbf{0.462} & 0.614 & 2.180 & 0.499 & 0.712 \\\\\n",
      "Univ & 1.576 & 1.369 & 1.280 & 1.205 & \\textbf{1.184} \\\\\n",
      "Zara 1 & 1.132 & 0.952 & 0.910 & \\textbf{0.770} & 1.357 \\\\\n",
      "Zara 2 & 0.860 & 0.724 & 1.110 & \\textbf{0.589} & 0.804 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_most_likely_fde.style.highlight_min(props='textbf:--rwrap', axis=1).format(precision=3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3ac08",
   "metadata": {},
   "source": [
    "### Single output - ADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7ff1e486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c9af8_row0_col3, #T_c9af8_row1_col3, #T_c9af8_row2_col3, #T_c9af8_row3_col3, #T_c9af8_row4_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c9af8_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CVM (8pt history)</th>\n",
       "      <th class=\"col_heading level0 col1\" >CVM (2pt history)</th>\n",
       "      <th class=\"col_heading level0 col2\" >Social-GAN</th>\n",
       "      <th class=\"col_heading level0 col3\" >Trajectron++</th>\n",
       "      <th class=\"col_heading level0 col4\" >Ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c9af8_level0_row0\" class=\"row_heading level0 row0\" >ETH</th>\n",
       "      <td id=\"T_c9af8_row0_col0\" class=\"data row0 col0\" >1.102</td>\n",
       "      <td id=\"T_c9af8_row0_col1\" class=\"data row0 col1\" >1.075</td>\n",
       "      <td id=\"T_c9af8_row0_col2\" class=\"data row0 col2\" >1.130</td>\n",
       "      <td id=\"T_c9af8_row0_col3\" class=\"data row0 col3\" >0.695</td>\n",
       "      <td id=\"T_c9af8_row0_col4\" class=\"data row0 col4\" >0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c9af8_level0_row1\" class=\"row_heading level0 row1\" >Hotel</th>\n",
       "      <td id=\"T_c9af8_row1_col0\" class=\"data row1 col0\" >0.243</td>\n",
       "      <td id=\"T_c9af8_row1_col1\" class=\"data row1 col1\" >0.319</td>\n",
       "      <td id=\"T_c9af8_row1_col2\" class=\"data row1 col2\" >1.010</td>\n",
       "      <td id=\"T_c9af8_row1_col3\" class=\"data row1 col3\" >0.224</td>\n",
       "      <td id=\"T_c9af8_row1_col4\" class=\"data row1 col4\" >0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c9af8_level0_row2\" class=\"row_heading level0 row2\" >Univ</th>\n",
       "      <td id=\"T_c9af8_row2_col0\" class=\"data row2 col0\" >0.781</td>\n",
       "      <td id=\"T_c9af8_row2_col1\" class=\"data row2 col1\" >0.618</td>\n",
       "      <td id=\"T_c9af8_row2_col2\" class=\"data row2 col2\" >0.600</td>\n",
       "      <td id=\"T_c9af8_row2_col3\" class=\"data row2 col3\" >0.468</td>\n",
       "      <td id=\"T_c9af8_row2_col4\" class=\"data row2 col4\" >0.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c9af8_level0_row3\" class=\"row_heading level0 row3\" >Zara 1</th>\n",
       "      <td id=\"T_c9af8_row3_col0\" class=\"data row3 col0\" >0.552</td>\n",
       "      <td id=\"T_c9af8_row3_col1\" class=\"data row3 col1\" >0.427</td>\n",
       "      <td id=\"T_c9af8_row3_col2\" class=\"data row3 col2\" >0.420</td>\n",
       "      <td id=\"T_c9af8_row3_col3\" class=\"data row3 col3\" >0.297</td>\n",
       "      <td id=\"T_c9af8_row3_col4\" class=\"data row3 col4\" >0.563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c9af8_level0_row4\" class=\"row_heading level0 row4\" >Zara 2</th>\n",
       "      <td id=\"T_c9af8_row4_col0\" class=\"data row4 col0\" >0.421</td>\n",
       "      <td id=\"T_c9af8_row4_col1\" class=\"data row4 col1\" >0.324</td>\n",
       "      <td id=\"T_c9af8_row4_col2\" class=\"data row4 col2\" >0.520</td>\n",
       "      <td id=\"T_c9af8_row4_col3\" class=\"data row4 col3\" >0.227</td>\n",
       "      <td id=\"T_c9af8_row4_col4\" class=\"data row4 col4\" >0.369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x137ed3b20>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_most_likely_ade.style.highlight_min(color = 'lightgreen', axis = 1).format(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "aee749cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "{} & {CVM (8pt history)} & {CVM (2pt history)} & {Social-GAN} & {Trajectron++} & {Ours} \\\\\n",
      "ETH & 1.102 & 1.075 & 1.130 & \\textbf{0.695} & 0.947 \\\\\n",
      "Hotel & 0.243 & 0.319 & 1.010 & \\textbf{0.224} & 0.334 \\\\\n",
      "Univ & 0.781 & 0.618 & 0.600 & \\textbf{0.468} & 0.552 \\\\\n",
      "Zara 1 & 0.552 & 0.427 & 0.420 & \\textbf{0.297} & 0.563 \\\\\n",
      "Zara 2 & 0.421 & 0.324 & 0.520 & \\textbf{0.227} & 0.369 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_most_likely_ade.style.highlight_min(props='textbf:--rwrap', axis=1).format(precision=3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2db62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36db3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
