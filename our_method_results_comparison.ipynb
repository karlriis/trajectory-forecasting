{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1740c3",
   "metadata": {},
   "source": [
    "# Results comparison between Trajectron++ and our method\n",
    "Comparing the results of our method with the Trajectron++ paper's method. The goal of the comparison is to evaluate Trajectron++ on a different dataset than it was trained on to allow more fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cfa130bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import generative_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd1a8c",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1ac04490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_FDE(pred_x, pred_y, test_x, test_y):\n",
    "\n",
    "    final_displacement_x = pred_x[-1] - test_x[-1]\n",
    "    final_displacement_y = pred_y[-1] - test_y[-1]\n",
    "    FDE = np.sqrt(final_displacement_x**2 + final_displacement_y**2)\n",
    "    \n",
    "    return FDE\n",
    "\n",
    "def calculate_ADE(pred_x, pred_y, test_x, test_y):\n",
    "    assert len(pred_x) == len(test_x)\n",
    "    total_displacement_error = 0\n",
    "    for point_idx in range(len(test_x)):\n",
    "        displacement_error = np.sqrt((pred_x[point_idx] - test_x[point_idx])**2 + (pred_y[point_idx] - test_y[point_idx])**2)\n",
    "        total_displacement_error += displacement_error\n",
    "\n",
    "    return total_displacement_error/len(pred_x)\n",
    "\n",
    "## The evaluation logic for Trajectron++ loops over the frames and predicts the future trajectories \n",
    "## for each node present in the current frame\n",
    "## Each node has to have at least 7 historical points and 12 future points\n",
    "def get_total_predictable_slices(data):\n",
    "    total_predictable_steps = 0\n",
    "    for i in pd.unique(data.node_id):\n",
    "        #print(len(test[test.node_id == i]))\n",
    "        total_predictable_steps += len(data[data.node_id == i]) - 19\n",
    "    return total_predictable_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "907d43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_data):\n",
    "    data = input_data.copy()\n",
    "    data['frame_id'] = pd.to_numeric(data['frame_id'], downcast='integer')\n",
    "    data['track_id'] = pd.to_numeric(data['track_id'], downcast='integer')\n",
    "\n",
    "    data['frame_id'] = data['frame_id'] // 10\n",
    "\n",
    "    data['frame_id'] -= data['frame_id'].min()\n",
    "\n",
    "    data['node_type'] = 'PEDESTRIAN'\n",
    "    data['node_id'] = data['track_id'].astype(str)\n",
    "    data.sort_values('frame_id', inplace=True)\n",
    "\n",
    "    data['pos_x'] = data['pos_x'] - data['pos_x'].mean()\n",
    "    data['pos_y'] = data['pos_y'] - data['pos_y'].mean()\n",
    "    \n",
    "    # Select only such nodes which have enough data to predict on (8 historical timesteps, 12 future)\n",
    "    v = data.node_id.value_counts()\n",
    "    data = data[data.node_id.isin(v.index[v.gt(19)])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce140413",
   "metadata": {},
   "source": [
    "## Method evaluation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "46e27d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_our_method(data, params, dataset_title='', clustering_method='KMeans', smoothing=False):\n",
    "    our_fde_best_of = []\n",
    "    our_ade_best_of = []\n",
    "    our_fde_single = []\n",
    "    our_ade_single = []\n",
    "\n",
    "    # Loop over the dataset frame by frame\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='Ours - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        \n",
    "        # Loop over all agents in the current frame\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check for 8 points of history for current agent\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Check for 12 points of future for current agent\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    all_pred_x, all_pred_y, weights = generative_model.predict(x_data, y_data, params, trajectory_length=12, clustering_method=clustering_method, smoothing=smoothing)\n",
    "                    assert len(all_pred_x[0]) == 12\n",
    "                    \n",
    "                    # This section is for producing a single trajectory (averaging all representative preds)\n",
    "                    avg_x = np.average(all_pred_x, axis=0, weights=weights)\n",
    "                    avg_y = np.average(all_pred_y, axis=0, weights=weights)\n",
    "                    #avg_x = np.mean(all_pred_x, axis=0)\n",
    "                    #avg_y = np.mean(all_pred_y, axis=0)\n",
    "                    avg_fde = calculate_FDE(avg_x, avg_y, x_gt, y_gt)\n",
    "                    avg_ade = calculate_ADE(avg_x, avg_y, x_gt, y_gt)\n",
    "                    our_fde_single.append(avg_fde)\n",
    "                    our_ade_single.append(avg_ade)\n",
    "                        \n",
    "                        \n",
    "                    # This section is for finding the best trajectories out of many in terms of FDE and ADE\n",
    "                    best_fde = None\n",
    "                    best_ade = None\n",
    "\n",
    "                    for i in range(len(all_pred_x)):\n",
    "                        current_pred_x = all_pred_x[i]\n",
    "                        current_pred_y = all_pred_y[i]\n",
    "\n",
    "                        fde = calculate_FDE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_fde == None or fde < best_fde:\n",
    "                            best_fde = fde\n",
    "\n",
    "                        ade = calculate_ADE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_ade == None or ade < best_ade:\n",
    "                            best_ade = ade\n",
    "\n",
    "                    our_fde_best_of.append(best_fde)\n",
    "                    our_ade_best_of.append(best_ade)\n",
    "                    \n",
    "    return our_fde_best_of, our_ade_best_of, our_fde_single, our_ade_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "781a48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cvm_with_scenarios(data, dataset_title='', history_length=8):\n",
    "    tot = 0\n",
    "    our_fde_best_of = []\n",
    "    our_ade_best_of = []\n",
    "\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='CVM - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        #print(frame_data)\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check if at least 7 historical points are present\n",
    "            # PS: It might be so that the prediction starts at the 8th step instead of 7th? Edited the code to do this at the moment\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Not sure why there has to be more than 12 frames to the future (at least 13) but it's the\n",
    "                # only way to get the number of trajectron++ eval predictions to match up\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    tot += 1\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    all_pred_x, all_pred_y = [], []\n",
    "                    \n",
    "                    # CVM\n",
    "                    for i in range(20):\n",
    "                        history_x = x_data[-history_length:]\n",
    "                        history_y = y_data[-history_length:]\n",
    "                        assert len(history_x) == history_length\n",
    "                        \n",
    "                        if i == 0:\n",
    "                            vel_x = [history_x[i] - history_x[i-1] for i in range(1, len(history_x))]\n",
    "                            vel_y = [history_y[i] - history_y[i-1] for i in range(1, len(history_y))]\n",
    "                        else:\n",
    "                            vel_x = [(history_x[i] - history_x[i-1]) + np.random.normal(0, 1) for i in range(1, len(history_x))]\n",
    "                            vel_y = [(history_y[i] - history_y[i-1]) + np.random.normal(0, 1) for i in range(1, len(history_y))]\n",
    "                        \n",
    "                        assert len(vel_x) == history_length-1\n",
    "                        avg_vel_x = np.mean(vel_x)\n",
    "                        avg_vel_y = np.mean(vel_y)\n",
    "                        \n",
    "                        pred_x = [x_data[-1] + i*avg_vel_x for i in range(1, 13)]\n",
    "                        pred_y = [y_data[-1] + i*avg_vel_y for i in range(1, 13)]\n",
    "                        assert len(pred_x) == 12\n",
    "                        \n",
    "                        all_pred_x.append(pred_x)\n",
    "                        all_pred_y.append(pred_y)\n",
    "                    \n",
    "\n",
    "                    best_fde = None\n",
    "                    best_ade = None\n",
    "                    for i in range(len(all_pred_x)):\n",
    "                        current_pred_x = all_pred_x[i]\n",
    "                        current_pred_y = all_pred_y[i]\n",
    "\n",
    "                        fde = calculate_FDE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_fde == None or fde < best_fde:\n",
    "                            best_fde = fde\n",
    "\n",
    "                        ade = calculate_ADE(current_pred_x, current_pred_y, x_gt, y_gt)\n",
    "                        if best_ade == None or ade < best_ade:\n",
    "                            best_ade = ade\n",
    "\n",
    "                    our_fde_best_of.append(best_fde)\n",
    "                    our_ade_best_of.append(best_ade)\n",
    "                    \n",
    "    return our_fde_best_of, our_ade_best_of\n",
    "\n",
    "def evaluate_cvm(data, dataset_title='', history_length=8):\n",
    "    tot = 0\n",
    "    all_fde = []\n",
    "    all_ade = []\n",
    "\n",
    "    for frame_id in tqdm(pd.unique(data.frame_id), desc='CVM - ' + dataset_title):\n",
    "\n",
    "        frame_data = data[data.frame_id == frame_id]\n",
    "        #print(frame_data)\n",
    "        for node_id in pd.unique(frame_data.node_id):\n",
    "            # Check if at least 7 historical points are present\n",
    "            # PS: It might be so that the prediction starts at the 8th step instead of 7th? Edited the code to do this at the moment\n",
    "            if len(data[((data.node_id == node_id) & (data.frame_id <= frame_id))]) >= 8:\n",
    "                # Not sure why there has to be more than 12 frames to the future (at least 13) but it's the\n",
    "                # only way to get the number of trajectron++ eval predictions to match up\n",
    "                if len(data[((data.node_id == node_id) & (data.frame_id > frame_id))]) >= 12:\n",
    "                    tot += 1\n",
    "                    node_history_data = data[((data.node_id == node_id) & (data.frame_id <= frame_id) & (data.frame_id >= frame_id-7))]\n",
    "                    node_gt_data = data[((data.node_id == node_id) & (data.frame_id > frame_id) & (data.frame_id <= frame_id+12))]\n",
    "\n",
    "                    x_data = list(node_history_data.pos_x)\n",
    "                    y_data = list(node_history_data.pos_y)\n",
    "                    assert len(x_data) == 8\n",
    "\n",
    "                    x_gt = list(node_gt_data.pos_x)\n",
    "                    y_gt = list(node_gt_data.pos_y)\n",
    "                    assert len(x_gt) == 12\n",
    "\n",
    "                    history_x = x_data[-history_length:]\n",
    "                    history_y = y_data[-history_length:]\n",
    "                    assert len(history_x) == history_length\n",
    "\n",
    "                    vel_x = [history_x[i] - history_x[i-1] for i in range(1, len(history_x))]\n",
    "                    vel_y = [history_y[i] - history_y[i-1] for i in range(1, len(history_y))]\n",
    "\n",
    "                    assert len(vel_x) == history_length-1\n",
    "                    avg_vel_x = np.mean(vel_x)\n",
    "                    avg_vel_y = np.mean(vel_y)\n",
    "\n",
    "                    pred_x = [x_data[-1] + i*avg_vel_x for i in range(1, 13)]\n",
    "                    pred_y = [y_data[-1] + i*avg_vel_y for i in range(1, 13)]\n",
    "                    assert len(pred_x) == 12\n",
    "\n",
    "                    fde = calculate_FDE(pred_x, pred_y, x_gt, y_gt)\n",
    "                    ade = calculate_ADE(pred_x, pred_y, x_gt, y_gt)\n",
    "\n",
    "                    all_fde.append(fde)\n",
    "                    all_ade.append(ade)\n",
    "                    \n",
    "    return all_fde, all_ade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a687f69",
   "metadata": {},
   "source": [
    "## Automated results comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ef2ab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trajectron_data(trajectron_resultset_name, base_folder='./trajectron++/results_paper_version/', suffix='best_of'):\n",
    "    trajectron_fde = []\n",
    "    with open(base_folder + trajectron_resultset_name + '_fde_' + suffix + '.csv', mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            trajectron_fde.append(float(row['value']))\n",
    "\n",
    "    trajectron_ade = []\n",
    "    with open(base_folder + trajectron_resultset_name + '_ade_'+ suffix + '.csv', mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            trajectron_ade.append(float(row['value']))\n",
    "            \n",
    "    return trajectron_fde, trajectron_ade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3cf14c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_datasets(our_method_params, datasets, trajectron_resultset_names, trajectron_ar3_resultset_names, evaluate_most_likely=False, evaluate_with_interactions=False):\n",
    "    base_path = './raw_data/'\n",
    "\n",
    "    ours_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    trajectron_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    trajectron_ar3_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    cvm_long_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "    cvm_short_results = {'BEST_OF_20': {'FDE': [], 'ADE': []}, 'MOST_LIKELY': {'FDE': [], 'ADE': []}}\n",
    "\n",
    "    for dataset_idx, dataset in enumerate(datasets):\n",
    "        our_fde_bo20, our_ade_bo20 = [], []\n",
    "        our_fde_most_likely, our_ade_most_likely = [], []\n",
    "        \n",
    "        cvm_fde_bo20, cvm_ade_bo20 = [], []\n",
    "        cvm_short_fde_bo20, cvm_short_ade_bo20 = [], []\n",
    "        \n",
    "        cvm_fde_ml, cvm_ade_ml = [], []\n",
    "        cvm_short_fde_ml, cvm_short_ade_ml = [], []\n",
    "        \n",
    "        for scene_idx, scene in enumerate(dataset):\n",
    "            data = pd.read_csv(base_path + scene, sep='\\t', index_col=False, header=None)\n",
    "            data.columns = ['frame_id', 'track_id', 'pos_x', 'pos_y']\n",
    "\n",
    "            data = process_data(data)\n",
    "\n",
    "            ## Ours\n",
    "            our_fde_best_of_20, our_ade_best_of_20, our_fde_single, our_ade_single = evaluate_our_method(data, our_method_params[dataset_idx], dataset_title=trajectron_resultset_names[dataset_idx], smoothing=True)\n",
    "            our_fde_bo20 += our_fde_best_of_20\n",
    "            our_ade_bo20 += our_ade_best_of_20\n",
    "            if evaluate_most_likely:\n",
    "                our_fde_most_likely += our_fde_single\n",
    "                our_ade_most_likely += our_ade_single\n",
    "\n",
    "            ## CVM\n",
    "            cvm_fde_best_of, cvm_ade_best_of = evaluate_cvm_with_scenarios(data, dataset_title=trajectron_resultset_names[dataset_idx])\n",
    "            cvm_fde_bo20 += cvm_fde_best_of\n",
    "            cvm_ade_bo20 += cvm_ade_best_of\n",
    "            \n",
    "            cvm_fde_short_history_best_of, cvm_ade_short_history_best_of = evaluate_cvm_with_scenarios(data, dataset_title=trajectron_resultset_names[dataset_idx], history_length=2)\n",
    "            cvm_short_fde_bo20 += cvm_fde_short_history_best_of\n",
    "            cvm_short_ade_bo20 += cvm_ade_short_history_best_of\n",
    "\n",
    "            if evaluate_most_likely:\n",
    "                cvm_fde, cvm_ade = evaluate_cvm(data, dataset_title=trajectron_resultset_names[dataset_idx])\n",
    "                cvm_fde_ml += cvm_fde\n",
    "                cvm_ade_ml += cvm_ade\n",
    "                \n",
    "                cvm_fde_short_history, cvm_ade_short_history = evaluate_cvm(data, dataset_title=trajectron_resultset_names[dataset_idx], history_length=2)\n",
    "                cvm_short_fde_ml += cvm_fde_short_history\n",
    "                cvm_short_ade_ml += cvm_ade_short_history\n",
    "\n",
    "        # add our and CVM results to the data dict\n",
    "        ours_results['BEST_OF_20']['FDE'].append(np.mean(our_fde_bo20))\n",
    "        ours_results['BEST_OF_20']['ADE'].append(np.mean(our_ade_bo20))\n",
    "        if evaluate_most_likely:\n",
    "            ours_results['MOST_LIKELY']['FDE'].append(np.mean(our_fde_most_likely))\n",
    "            ours_results['MOST_LIKELY']['ADE'].append(np.mean(our_ade_most_likely))\n",
    "            \n",
    "        cvm_long_results['BEST_OF_20']['FDE'].append(np.mean(cvm_fde_best_of))\n",
    "        cvm_long_results['BEST_OF_20']['ADE'].append(np.mean(cvm_ade_best_of))\n",
    "        cvm_short_results['BEST_OF_20']['FDE'].append(np.mean(cvm_fde_short_history_best_of))\n",
    "        cvm_short_results['BEST_OF_20']['ADE'].append(np.mean(cvm_ade_short_history_best_of))\n",
    "        if evaluate_most_likely:\n",
    "            cvm_long_results['MOST_LIKELY']['FDE'].append(np.mean(cvm_fde))\n",
    "            cvm_long_results['MOST_LIKELY']['ADE'].append(np.mean(cvm_ade))\n",
    "            cvm_short_results['MOST_LIKELY']['FDE'].append(np.mean(cvm_fde_short_history))\n",
    "            cvm_short_results['MOST_LIKELY']['ADE'].append(np.mean(cvm_ade_short_history))\n",
    "                \n",
    "        ## Trajectron\n",
    "        trajectron_fde, trajectron_ade = read_trajectron_data(trajectron_resultset_names[dataset_idx])\n",
    "        trajectron_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_fde))\n",
    "        trajectron_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ade))\n",
    "        \n",
    "        if evaluate_most_likely:\n",
    "            trajectron_fde, trajectron_ade = read_trajectron_data(trajectron_resultset_names[dataset_idx], suffix='most_likely')\n",
    "            trajectron_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_fde))\n",
    "            trajectron_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ade))\n",
    "        \n",
    "        ## Trajectron with interactions\n",
    "        if evaluate_with_interactions:\n",
    "            trajectron_ar3_fde, trajectron_ar3_ade = read_trajectron_data(trajectron_ar3_resultset_names[dataset_idx])\n",
    "            trajectron_ar3_results['BEST_OF_20']['FDE'].append(np.mean(trajectron_ar3_fde))\n",
    "            trajectron_ar3_results['BEST_OF_20']['ADE'].append(np.mean(trajectron_ar3_ade))\n",
    "\n",
    "            if evaluate_most_likely:\n",
    "                trajectron_ar3_fde, trajectron_ar3_ade = read_trajectron_data(trajectron_resultset_names[dataset_idx], suffix='most_likely')\n",
    "                trajectron_ar3_results['MOST_LIKELY']['FDE'].append(np.mean(trajectron_ar3_fde))\n",
    "                trajectron_ar3_results['MOST_LIKELY']['ADE'].append(np.mean(trajectron_ar3_ade))\n",
    "\n",
    "        # make sure that there is no discrepancy between our data processing and trajectron evaluation results size\n",
    "        if len(dataset) == 1:\n",
    "            num_predictable_trajectories = get_total_predictable_slices(data)\n",
    "            assert len(trajectron_fde) == num_predictable_trajectories\n",
    "            assert len(trajectron_ade) == num_predictable_trajectories\n",
    "    \n",
    "    return [\n",
    "        ours_results,\n",
    "        trajectron_results,\n",
    "        trajectron_ar3_results,\n",
    "        cvm_long_results,\n",
    "        cvm_short_results\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac9d82",
   "metadata": {},
   "source": [
    "### Running the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a429d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ours - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:23<00:00, 20.17it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:01<00:00, 468.25it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 525.19it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 696.86it/s]\n",
      "CVM - eth_vel: 100%|█████████████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 690.39it/s]\n",
      "Ours - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [01:12<00:00, 12.54it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [00:03<00:00, 241.67it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [00:03<00:00, 268.75it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [00:02<00:00, 337.69it/s]\n",
      "CVM - hotel_vel: 100%|███████████████████████████████████████████████████████████████████████| 913/913 [00:02<00:00, 334.97it/s]\n",
      "Ours - univ_vel: 100%|████████████████████████████████████████████████████████████████████████| 444/444 [18:21<00:00,  2.48s/it]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 444/444 [01:18<00:00,  5.62it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 444/444 [01:14<00:00,  5.95it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 444/444 [01:05<00:00,  6.75it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 444/444 [01:05<00:00,  6.74it/s]\n",
      "Ours - univ_vel: 100%|████████████████████████████████████████████████████████████████████████| 541/541 [12:19<00:00,  1.37s/it]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 541/541 [00:51<00:00, 10.50it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 541/541 [00:48<00:00, 11.24it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 541/541 [00:42<00:00, 12.80it/s]\n",
      "CVM - univ_vel: 100%|█████████████████████████████████████████████████████████████████████████| 541/541 [00:42<00:00, 12.80it/s]\n",
      "Ours - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [02:38<00:00,  5.44it/s]\n",
      "CVM - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [00:07<00:00, 115.04it/s]\n",
      "CVM - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [00:06<00:00, 127.74it/s]\n",
      "CVM - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [00:05<00:00, 160.65it/s]\n",
      "CVM - zara1_vel_modified: 100%|██████████████████████████████████████████████████████████████| 864/864 [00:05<00:00, 161.22it/s]\n",
      "Ours - zara2_vel_modified: 100%|████████████████████████████████████████████████████████████| 1052/1052 [05:55<00:00,  2.96it/s]\n",
      "CVM - zara2_vel_modified: 100%|█████████████████████████████████████████████████████████████| 1052/1052 [00:20<00:00, 51.76it/s]\n",
      "CVM - zara2_vel_modified: 100%|█████████████████████████████████████████████████████████████| 1052/1052 [00:18<00:00, 56.60it/s]\n",
      "CVM - zara2_vel_modified: 100%|█████████████████████████████████████████████████████████████| 1052/1052 [00:15<00:00, 68.99it/s]\n",
      "CVM - zara2_vel_modified: 100%|█████████████████████████████████████████████████████████████| 1052/1052 [00:15<00:00, 68.98it/s]\n"
     ]
    }
   ],
   "source": [
    "eth_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 300, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.05, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.15, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.5, 0.75, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 9, 6, 4]\n",
    "}\n",
    "\n",
    "hotel_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 300, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.05, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.1, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 11, 8]\n",
    "}\n",
    "\n",
    "zara1_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 300, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.05, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.2, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 14, 5]\n",
    "}\n",
    "\n",
    "zara2_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 300, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.05, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.2, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.1, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.60, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 14, 5]\n",
    "}\n",
    "\n",
    "univ_params = {\n",
    "    'NOISE': 0.05, \n",
    "    'NO_OF_TRAJECTORIES': 300, \n",
    "    'CONST_VEL_MODEL_PROB': 0.5, \n",
    "    'STOP_PROB': 0.05, \n",
    "    'DISCOUNT_AVG_PROB': 1.0, \n",
    "    'DISCOUNT_LOWER_BOUND': 0.1, \n",
    "    'VELOCITY_CHANGE_PROB': 0.1,\n",
    "    'VELOCITY_CHANGE_NOISE': 0.1, \n",
    "    'ANGLE_CHANGE_PROB': 0.2, \n",
    "    'ANGLE_CHANGE_NOISE': 2, \n",
    "    'GROUP_PERCENTAGES': [0.05, 0.5, 0.75, 1.0], \n",
    "    'GROUP_CLUSTER_COUNT': [1, 9, 6, 4]\n",
    "}\n",
    "\n",
    "\n",
    "our_method_params = [eth_params, hotel_params, univ_params, zara1_params, zara2_params]\n",
    "\n",
    "datasets = [\n",
    "    ['eth/test/biwi_eth.txt'], \n",
    "    ['hotel/test/biwi_hotel.txt'], \n",
    "    ['univ/test/students001.txt', 'univ/test/students003.txt'],\n",
    "    ['zara1/test/crowds_zara01.txt'], \n",
    "    ['zara2/test/crowds_zara02.txt'],\n",
    "]\n",
    "\n",
    "trajectron_resultset_names = [\n",
    "    'eth_vel', \n",
    "    'hotel_vel',\n",
    "    'univ_vel',\n",
    "    'zara1_vel_modified', \n",
    "    'zara2_vel_modified',\n",
    "]\n",
    "\n",
    "trajectron_ar3_resultset_names = [\n",
    "    'eth_ar3', \n",
    "    'hotel_ar3', \n",
    "    'univ_ar3',\n",
    "    'zara1_ar3', \n",
    "    'zara2_ar3',\n",
    "]\n",
    "\n",
    "res = evaluate_all_datasets(our_method_params, datasets, trajectron_resultset_names, trajectron_ar3_resultset_names, evaluate_most_likely=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8bedb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ours_results = res[0]\n",
    "trajectron_results = res[1]\n",
    "trajectron_ar3_results = res[2]\n",
    "cvm_long_results = res[3]\n",
    "cvm_short_results = res[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7260ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgan_results = {\n",
    "    'BEST_OF_20': {\n",
    "        'FDE': [1.52, 1.61, 0.69, 1.26, 0.84], \n",
    "        'ADE': [0.81, 0.72, 0.60, 0.34, 0.42]\n",
    "    }, \n",
    "    'MOST_LIKELY': {\n",
    "        'FDE': [2.21, 2.18, 1.28, 0.91, 1.11], \n",
    "        'ADE': [ 1.13, 1.01, 0.60, 0.42, 0.52]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ab25de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [\n",
    "    'ETH', \n",
    "    'Hotel', \n",
    "    'Univ',\n",
    "    'Zara 1', \n",
    "    'Zara 2',\n",
    "    \n",
    "]\n",
    "\n",
    "df_data_best_of_20_fde = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Social-GAN': pd.Series(sgan_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['BEST_OF_20']['FDE'], index = index),\n",
    "    #'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['BEST_OF_20']['FDE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['BEST_OF_20']['FDE'], index = index)\n",
    "}\n",
    "\n",
    "df_best_of_20_fde = pd.DataFrame(df_data_best_of_20_fde)\n",
    "\n",
    "df_data_best_of_20_ade = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Social-GAN': pd.Series(sgan_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['BEST_OF_20']['ADE'], index = index),\n",
    "    #'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['BEST_OF_20']['ADE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['BEST_OF_20']['ADE'], index = index)\n",
    "}\n",
    "\n",
    "df_best_of_20_ade = pd.DataFrame(df_data_best_of_20_ade)\n",
    "\n",
    "df_data_most_likely_fde = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Social-GAN': pd.Series(sgan_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    #'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['MOST_LIKELY']['FDE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['MOST_LIKELY']['FDE'], index = index)\n",
    "}\n",
    "\n",
    "df_most_likely_fde = pd.DataFrame(df_data_most_likely_fde)\n",
    "\n",
    "df_data_most_likely_ade = {\n",
    "    'CVM (8pt history)' : pd.Series(cvm_long_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'CVM (2pt history)' : pd.Series(cvm_short_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Social-GAN': pd.Series(sgan_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Trajectron++' : pd.Series(trajectron_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    #'Trajectron++ AR3' : pd.Series(trajectron_ar3_results['MOST_LIKELY']['ADE'], index = index),\n",
    "    'Ours': pd.Series(ours_results['MOST_LIKELY']['ADE'], index = index)\n",
    "}\n",
    "\n",
    "df_most_likely_ade = pd.DataFrame(df_data_most_likely_ade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64a9ed",
   "metadata": {},
   "source": [
    "### Best of 20 - FDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4c1078db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fd9c8_row0_col4, #T_fd9c8_row1_col4, #T_fd9c8_row2_col4, #T_fd9c8_row3_col3, #T_fd9c8_row4_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fd9c8_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CVM (8pt history)</th>\n",
       "      <th class=\"col_heading level0 col1\" >CVM (2pt history)</th>\n",
       "      <th class=\"col_heading level0 col2\" >Social-GAN</th>\n",
       "      <th class=\"col_heading level0 col3\" >Trajectron++</th>\n",
       "      <th class=\"col_heading level0 col4\" >Ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9c8_level0_row0\" class=\"row_heading level0 row0\" >ETH</th>\n",
       "      <td id=\"T_fd9c8_row0_col0\" class=\"data row0 col0\" >1.091436</td>\n",
       "      <td id=\"T_fd9c8_row0_col1\" class=\"data row0 col1\" >1.698870</td>\n",
       "      <td id=\"T_fd9c8_row0_col2\" class=\"data row0 col2\" >1.520000</td>\n",
       "      <td id=\"T_fd9c8_row0_col3\" class=\"data row0 col3\" >0.812384</td>\n",
       "      <td id=\"T_fd9c8_row0_col4\" class=\"data row0 col4\" >0.659920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9c8_level0_row1\" class=\"row_heading level0 row1\" >Hotel</th>\n",
       "      <td id=\"T_fd9c8_row1_col0\" class=\"data row1 col0\" >0.358949</td>\n",
       "      <td id=\"T_fd9c8_row1_col1\" class=\"data row1 col1\" >0.581098</td>\n",
       "      <td id=\"T_fd9c8_row1_col2\" class=\"data row1 col2\" >1.610000</td>\n",
       "      <td id=\"T_fd9c8_row1_col3\" class=\"data row1 col3\" >0.197178</td>\n",
       "      <td id=\"T_fd9c8_row1_col4\" class=\"data row1 col4\" >0.185460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9c8_level0_row2\" class=\"row_heading level0 row2\" >Univ</th>\n",
       "      <td id=\"T_fd9c8_row2_col0\" class=\"data row2 col0\" >0.966364</td>\n",
       "      <td id=\"T_fd9c8_row2_col1\" class=\"data row2 col1\" >1.206381</td>\n",
       "      <td id=\"T_fd9c8_row2_col2\" class=\"data row2 col2\" >0.690000</td>\n",
       "      <td id=\"T_fd9c8_row2_col3\" class=\"data row2 col3\" >0.450190</td>\n",
       "      <td id=\"T_fd9c8_row2_col4\" class=\"data row2 col4\" >0.445767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9c8_level0_row3\" class=\"row_heading level0 row3\" >Zara 1</th>\n",
       "      <td id=\"T_fd9c8_row3_col0\" class=\"data row3 col0\" >0.788392</td>\n",
       "      <td id=\"T_fd9c8_row3_col1\" class=\"data row3 col1\" >0.885519</td>\n",
       "      <td id=\"T_fd9c8_row3_col2\" class=\"data row3 col2\" >1.260000</td>\n",
       "      <td id=\"T_fd9c8_row3_col3\" class=\"data row3 col3\" >0.341834</td>\n",
       "      <td id=\"T_fd9c8_row3_col4\" class=\"data row3 col4\" >0.411453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9c8_level0_row4\" class=\"row_heading level0 row4\" >Zara 2</th>\n",
       "      <td id=\"T_fd9c8_row4_col0\" class=\"data row4 col0\" >0.553935</td>\n",
       "      <td id=\"T_fd9c8_row4_col1\" class=\"data row4 col1\" >0.651882</td>\n",
       "      <td id=\"T_fd9c8_row4_col2\" class=\"data row4 col2\" >0.840000</td>\n",
       "      <td id=\"T_fd9c8_row4_col3\" class=\"data row4 col3\" >0.253352</td>\n",
       "      <td id=\"T_fd9c8_row4_col4\" class=\"data row4 col4\" >0.304201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1377e7f40>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-means without smoothing\n",
    "df_best_of_20_fde.style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "77b1d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} & {CVM (8pt history)} & {CVM (2pt history)} & {Social-GAN} & {Trajectron++} & {Ours} \\\\\n",
      "\\midrule\n",
      "ETH & 1.091436 & 1.698870 & 1.520000 & 0.812384 & \\textbf{0.659920} \\\\\n",
      "Hotel & 0.358949 & 0.581098 & 1.610000 & 0.197178 & \\textbf{0.185460} \\\\\n",
      "Univ & 0.966364 & 1.206381 & 0.690000 & 0.450190 & \\textbf{0.445767} \\\\\n",
      "Zara 1 & 0.788392 & 0.885519 & 1.260000 & \\textbf{0.341834} & 0.411453 \\\\\n",
      "Zara 2 & 0.553935 & 0.651882 & 0.840000 & \\textbf{0.253352} & 0.304201 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(df_best_of_20_fde.to_latex(header=False))\n",
    "\n",
    "print(df_best_of_20_fde.style.highlight_min(props='textbf:--rwrap', axis=1).to_latex(hrules=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296114d",
   "metadata": {},
   "source": [
    "### Best of 20 - ADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "013fcd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bde4f_row0_col3, #T_bde4f_row1_col3, #T_bde4f_row2_col3, #T_bde4f_row3_col3, #T_bde4f_row4_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bde4f_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CVM (8pt history)</th>\n",
       "      <th class=\"col_heading level0 col1\" >CVM (2pt history)</th>\n",
       "      <th class=\"col_heading level0 col2\" >Social-GAN</th>\n",
       "      <th class=\"col_heading level0 col3\" >Trajectron++</th>\n",
       "      <th class=\"col_heading level0 col4\" >Ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bde4f_level0_row0\" class=\"row_heading level0 row0\" >ETH</th>\n",
       "      <td id=\"T_bde4f_row0_col0\" class=\"data row0 col0\" >0.635120</td>\n",
       "      <td id=\"T_bde4f_row0_col1\" class=\"data row0 col1\" >0.868997</td>\n",
       "      <td id=\"T_bde4f_row0_col2\" class=\"data row0 col2\" >0.810000</td>\n",
       "      <td id=\"T_bde4f_row0_col3\" class=\"data row0 col3\" >0.396031</td>\n",
       "      <td id=\"T_bde4f_row0_col4\" class=\"data row0 col4\" >0.438772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bde4f_level0_row1\" class=\"row_heading level0 row1\" >Hotel</th>\n",
       "      <td id=\"T_bde4f_row1_col0\" class=\"data row1 col0\" >0.203495</td>\n",
       "      <td id=\"T_bde4f_row1_col1\" class=\"data row1 col1\" >0.309259</td>\n",
       "      <td id=\"T_bde4f_row1_col2\" class=\"data row1 col2\" >0.720000</td>\n",
       "      <td id=\"T_bde4f_row1_col3\" class=\"data row1 col3\" >0.115423</td>\n",
       "      <td id=\"T_bde4f_row1_col4\" class=\"data row1 col4\" >0.125524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bde4f_level0_row2\" class=\"row_heading level0 row2\" >Univ</th>\n",
       "      <td id=\"T_bde4f_row2_col0\" class=\"data row2 col0\" >0.525410</td>\n",
       "      <td id=\"T_bde4f_row2_col1\" class=\"data row2 col1\" >0.570101</td>\n",
       "      <td id=\"T_bde4f_row2_col2\" class=\"data row2 col2\" >0.600000</td>\n",
       "      <td id=\"T_bde4f_row2_col3\" class=\"data row2 col3\" >0.204697</td>\n",
       "      <td id=\"T_bde4f_row2_col4\" class=\"data row2 col4\" >0.263688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bde4f_level0_row3\" class=\"row_heading level0 row3\" >Zara 1</th>\n",
       "      <td id=\"T_bde4f_row3_col0\" class=\"data row3 col0\" >0.411510</td>\n",
       "      <td id=\"T_bde4f_row3_col1\" class=\"data row3 col1\" >0.407797</td>\n",
       "      <td id=\"T_bde4f_row3_col2\" class=\"data row3 col2\" >0.340000</td>\n",
       "      <td id=\"T_bde4f_row3_col3\" class=\"data row3 col3\" >0.154697</td>\n",
       "      <td id=\"T_bde4f_row3_col4\" class=\"data row3 col4\" >0.241797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bde4f_level0_row4\" class=\"row_heading level0 row4\" >Zara 2</th>\n",
       "      <td id=\"T_bde4f_row4_col0\" class=\"data row4 col0\" >0.296717</td>\n",
       "      <td id=\"T_bde4f_row4_col1\" class=\"data row4 col1\" >0.303092</td>\n",
       "      <td id=\"T_bde4f_row4_col2\" class=\"data row4 col2\" >0.420000</td>\n",
       "      <td id=\"T_bde4f_row4_col3\" class=\"data row4 col3\" >0.115362</td>\n",
       "      <td id=\"T_bde4f_row4_col4\" class=\"data row4 col4\" >0.183251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x164069b20>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-means without smoothing\n",
    "df_best_of_20_ade.style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d3d1fc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "{} & {CVM (8pt history)} & {CVM (2pt history)} & {Social-GAN} & {Trajectron++} & {Ours} \\\\\n",
      "ETH & 0.635120 & 0.868997 & 0.810000 & \\textbf{0.396031} & 0.438772 \\\\\n",
      "Hotel & 0.203495 & 0.309259 & 0.720000 & \\textbf{0.115423} & 0.125524 \\\\\n",
      "Univ & 0.525410 & 0.570101 & 0.600000 & \\textbf{0.204697} & 0.263688 \\\\\n",
      "Zara 1 & 0.411510 & 0.407797 & 0.340000 & \\textbf{0.154697} & 0.241797 \\\\\n",
      "Zara 2 & 0.296717 & 0.303092 & 0.420000 & \\textbf{0.115362} & 0.183251 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_best_of_20_ade.style.highlight_min(props='textbf:--rwrap', axis=1).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee6f79",
   "metadata": {},
   "source": [
    "### Single output - FDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0c3796e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5c091_row0_col3, #T_5c091_row1_col0, #T_5c091_row2_col4, #T_5c091_row3_col3, #T_5c091_row4_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5c091_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CVM (8pt history)</th>\n",
       "      <th class=\"col_heading level0 col1\" >CVM (2pt history)</th>\n",
       "      <th class=\"col_heading level0 col2\" >Social-GAN</th>\n",
       "      <th class=\"col_heading level0 col3\" >Trajectron++</th>\n",
       "      <th class=\"col_heading level0 col4\" >Ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5c091_level0_row0\" class=\"row_heading level0 row0\" >ETH</th>\n",
       "      <td id=\"T_5c091_row0_col0\" class=\"data row0 col0\" >2.303287</td>\n",
       "      <td id=\"T_5c091_row0_col1\" class=\"data row0 col1\" >2.281890</td>\n",
       "      <td id=\"T_5c091_row0_col2\" class=\"data row0 col2\" >2.210000</td>\n",
       "      <td id=\"T_5c091_row0_col3\" class=\"data row0 col3\" >1.615308</td>\n",
       "      <td id=\"T_5c091_row0_col4\" class=\"data row0 col4\" >1.911543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5c091_level0_row1\" class=\"row_heading level0 row1\" >Hotel</th>\n",
       "      <td id=\"T_5c091_row1_col0\" class=\"data row1 col0\" >0.462310</td>\n",
       "      <td id=\"T_5c091_row1_col1\" class=\"data row1 col1\" >0.614198</td>\n",
       "      <td id=\"T_5c091_row1_col2\" class=\"data row1 col2\" >2.180000</td>\n",
       "      <td id=\"T_5c091_row1_col3\" class=\"data row1 col3\" >0.498741</td>\n",
       "      <td id=\"T_5c091_row1_col4\" class=\"data row1 col4\" >0.752817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5c091_level0_row2\" class=\"row_heading level0 row2\" >Univ</th>\n",
       "      <td id=\"T_5c091_row2_col0\" class=\"data row2 col0\" >1.576302</td>\n",
       "      <td id=\"T_5c091_row2_col1\" class=\"data row2 col1\" >1.368753</td>\n",
       "      <td id=\"T_5c091_row2_col2\" class=\"data row2 col2\" >1.280000</td>\n",
       "      <td id=\"T_5c091_row2_col3\" class=\"data row2 col3\" >1.204670</td>\n",
       "      <td id=\"T_5c091_row2_col4\" class=\"data row2 col4\" >1.202177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5c091_level0_row3\" class=\"row_heading level0 row3\" >Zara 1</th>\n",
       "      <td id=\"T_5c091_row3_col0\" class=\"data row3 col0\" >1.131947</td>\n",
       "      <td id=\"T_5c091_row3_col1\" class=\"data row3 col1\" >0.952377</td>\n",
       "      <td id=\"T_5c091_row3_col2\" class=\"data row3 col2\" >0.910000</td>\n",
       "      <td id=\"T_5c091_row3_col3\" class=\"data row3 col3\" >0.769595</td>\n",
       "      <td id=\"T_5c091_row3_col4\" class=\"data row3 col4\" >1.466909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5c091_level0_row4\" class=\"row_heading level0 row4\" >Zara 2</th>\n",
       "      <td id=\"T_5c091_row4_col0\" class=\"data row4 col0\" >0.859903</td>\n",
       "      <td id=\"T_5c091_row4_col1\" class=\"data row4 col1\" >0.724414</td>\n",
       "      <td id=\"T_5c091_row4_col2\" class=\"data row4 col2\" >1.110000</td>\n",
       "      <td id=\"T_5c091_row4_col3\" class=\"data row4 col3\" >0.589387</td>\n",
       "      <td id=\"T_5c091_row4_col4\" class=\"data row4 col4\" >0.788144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x137f762e0>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_most_likely_fde.style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "517bd291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "{} & {CVM (8pt history)} & {CVM (2pt history)} & {Social-GAN} & {Trajectron++} & {Ours} \\\\\n",
      "ETH & 2.303287 & 2.281890 & 2.210000 & \\textbf{1.615308} & 1.911543 \\\\\n",
      "Hotel & \\textbf{0.462310} & 0.614198 & 2.180000 & 0.498741 & 0.752817 \\\\\n",
      "Univ & 1.576302 & 1.368753 & 1.280000 & 1.204670 & \\textbf{1.202177} \\\\\n",
      "Zara 1 & 1.131947 & 0.952377 & 0.910000 & \\textbf{0.769595} & 1.466909 \\\\\n",
      "Zara 2 & 0.859903 & 0.724414 & 1.110000 & \\textbf{0.589387} & 0.788144 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_most_likely_fde.style.highlight_min(props='textbf:--rwrap', axis=1).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3ac08",
   "metadata": {},
   "source": [
    "### Single output - ADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7ff1e486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3dba2_row0_col3, #T_3dba2_row1_col3, #T_3dba2_row2_col3, #T_3dba2_row3_col3, #T_3dba2_row4_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3dba2_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >CVM (8pt history)</th>\n",
       "      <th class=\"col_heading level0 col1\" >CVM (2pt history)</th>\n",
       "      <th class=\"col_heading level0 col2\" >Social-GAN</th>\n",
       "      <th class=\"col_heading level0 col3\" >Trajectron++</th>\n",
       "      <th class=\"col_heading level0 col4\" >Ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba2_level0_row0\" class=\"row_heading level0 row0\" >ETH</th>\n",
       "      <td id=\"T_3dba2_row0_col0\" class=\"data row0 col0\" >1.101935</td>\n",
       "      <td id=\"T_3dba2_row0_col1\" class=\"data row0 col1\" >1.075458</td>\n",
       "      <td id=\"T_3dba2_row0_col2\" class=\"data row0 col2\" >1.130000</td>\n",
       "      <td id=\"T_3dba2_row0_col3\" class=\"data row0 col3\" >0.694815</td>\n",
       "      <td id=\"T_3dba2_row0_col4\" class=\"data row0 col4\" >0.947190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba2_level0_row1\" class=\"row_heading level0 row1\" >Hotel</th>\n",
       "      <td id=\"T_3dba2_row1_col0\" class=\"data row1 col0\" >0.243328</td>\n",
       "      <td id=\"T_3dba2_row1_col1\" class=\"data row1 col1\" >0.319356</td>\n",
       "      <td id=\"T_3dba2_row1_col2\" class=\"data row1 col2\" >1.010000</td>\n",
       "      <td id=\"T_3dba2_row1_col3\" class=\"data row1 col3\" >0.224216</td>\n",
       "      <td id=\"T_3dba2_row1_col4\" class=\"data row1 col4\" >0.354401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba2_level0_row2\" class=\"row_heading level0 row2\" >Univ</th>\n",
       "      <td id=\"T_3dba2_row2_col0\" class=\"data row2 col0\" >0.781078</td>\n",
       "      <td id=\"T_3dba2_row2_col1\" class=\"data row2 col1\" >0.618222</td>\n",
       "      <td id=\"T_3dba2_row2_col2\" class=\"data row2 col2\" >0.600000</td>\n",
       "      <td id=\"T_3dba2_row2_col3\" class=\"data row2 col3\" >0.467850</td>\n",
       "      <td id=\"T_3dba2_row2_col4\" class=\"data row2 col4\" >0.562417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba2_level0_row3\" class=\"row_heading level0 row3\" >Zara 1</th>\n",
       "      <td id=\"T_3dba2_row3_col0\" class=\"data row3 col0\" >0.551512</td>\n",
       "      <td id=\"T_3dba2_row3_col1\" class=\"data row3 col1\" >0.427223</td>\n",
       "      <td id=\"T_3dba2_row3_col2\" class=\"data row3 col2\" >0.420000</td>\n",
       "      <td id=\"T_3dba2_row3_col3\" class=\"data row3 col3\" >0.297096</td>\n",
       "      <td id=\"T_3dba2_row3_col4\" class=\"data row3 col4\" >0.622456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3dba2_level0_row4\" class=\"row_heading level0 row4\" >Zara 2</th>\n",
       "      <td id=\"T_3dba2_row4_col0\" class=\"data row4 col0\" >0.421038</td>\n",
       "      <td id=\"T_3dba2_row4_col1\" class=\"data row4 col1\" >0.323937</td>\n",
       "      <td id=\"T_3dba2_row4_col2\" class=\"data row4 col2\" >0.520000</td>\n",
       "      <td id=\"T_3dba2_row4_col3\" class=\"data row4 col3\" >0.227388</td>\n",
       "      <td id=\"T_3dba2_row4_col4\" class=\"data row4 col4\" >0.371437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1640691f0>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_most_likely_ade.style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "887b68d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "{} & {CVM (8pt history)} & {CVM (2pt history)} & {Social-GAN} & {Trajectron++} & {Ours} \\\\\n",
      "ETH & 1.101935 & 1.075458 & 1.130000 & \\textbf{0.694815} & 0.947190 \\\\\n",
      "Hotel & 0.243328 & 0.319356 & 1.010000 & \\textbf{0.224216} & 0.354401 \\\\\n",
      "Univ & 0.781078 & 0.618222 & 0.600000 & \\textbf{0.467850} & 0.562417 \\\\\n",
      "Zara 1 & 0.551512 & 0.427223 & 0.420000 & \\textbf{0.297096} & 0.622456 \\\\\n",
      "Zara 2 & 0.421038 & 0.323937 & 0.520000 & \\textbf{0.227388} & 0.371437 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_most_likely_ade.style.highlight_min(props='textbf:--rwrap', axis=1).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a040c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83dd14f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
